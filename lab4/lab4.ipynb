{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marcin Wardyński  \n",
    "czwartek, 8:00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zgodnie z poleceniem przetestuję następujące zdania z maskowaniem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = {\"PRZYPADKI\": [\n",
    "        \"Ta >>><<< jest bardzo mocno napompowana.\",\n",
    "        \"Naucz się >>><<< na jutrzejszy sprawdzian.\",\n",
    "        \"Prawdziwemu >>><<< zawsze można ufać.\",\n",
    "        \"W deszczowy dzień zabierz ze sobą >>><<<.\",\n",
    "        \"Uwielbiam spędzać czas z >>><<<.\",\n",
    "        \"Opowiadanie o >>><<< zawsze doprowadza mnie do łez.\",\n",
    "        \"Szanowna >>><<<, nawiązując do otrzymanych zastrzeżeń...\"\n",
    "    ],\n",
    "    \"RELACJA\": [\n",
    "        \"Gdy odbierasz paczkę, sprawdź >>><<< jeszcze przy kurierze.\",\n",
    "        \"Mijając ludzi na ulicy nie przyglądaj się >>><<< zbyt natarczywie.\",\n",
    "        \"Gdy już go znajdziesz, zrób z >>><<< co uważasz za stosowne.\"\n",
    "    ],\n",
    "    \"WIEDZA\": [\n",
    "        \"Stolicą Niemiec jest >>><<<.\",\n",
    "        \"Ziemia jest >>><<< planetą od Słońca.\",\n",
    "        \">>><<< jest procesem zmiany ze stanu ciekłego w stan gazowy.\",\n",
    "        \"Wisła uchodzi do >>><<<.\"\n",
    "    ],\n",
    "    \"NACECHOWANIE\": [\n",
    "        \"'Jestem w niej zakochany na zabój'  jest zdaniem wyrażającym >>><<<.\",\n",
    "        \"'Motyla noga, nigdy nie znajdę już tego świerszcza' powiedziała osoba o nastawieniu >>><<<.\",\n",
    "        \"'Jak ja nie cierpię tych okropnych Smerfów' to zdanie przedstawiające >>><<< emocje.\",\n",
    "        \"'Hura, nareszcie zdałem do kolejnej klasy' powiedziała osoba czująca się >>><<<.\",\n",
    "        \"'Niech on tylko wpadnie w moje ręce' spośród emocji pozytywna, neutralna i negatywna, najbliżej temu zdaniu jest emocja >>><<<.\"\n",
    "    ]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do sprawnego odpytywania modeli językowych o ich odpowiedz posłuży mi poniższa funkcja:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def evaluate_lm(cat_sentences, model):\n",
    "    mask_holder = \">>><<<\"\n",
    "    unmasker = pipeline('fill-mask', model=model, device=0)\n",
    "    \n",
    "    for cat, sentences in cat_sentences.items():\n",
    "        print()\n",
    "        for sentence in sentences:\n",
    "            masked_sentence = sentence.replace(mask_holder, unmasker.tokenizer.mask_token)\n",
    "            unmasked_tokens = unmasker(masked_sentence)\n",
    "            tokens = [token[\"token_str\"] for token in unmasked_tokens]\n",
    "            print(f\"{cat}: \\\"{sentence}\\\" <- {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W teście wezmą cztery modele językowe z huggingface.com:\n",
    "- https://huggingface.co/dkleczek/bert-base-polish-cased-v1\n",
    "- https://huggingface.co/jhu-clsp/bernice\n",
    "- https://huggingface.co/deepsense-ai/trelbert\n",
    "- https://huggingface.co/FacebookAI/xlm-roberta-base\n",
    "\n",
    "\n",
    "W ramach ćwiczenia sprawdzilem również następujące modele:\n",
    "\n",
    "- https://huggingface.co/google-bert/bert-base-multilingual-uncased\n",
    "- https://huggingface.co/Twitter/twhin-bert-base\n",
    "- https://huggingface.co/FacebookAI/xlm-mlm-17-1280\n",
    "- https://huggingface.co/studio-ousia/mluke-base\n",
    "- https://huggingface.co/sdadas/polish-longformer-base-4096\n",
    "- https://huggingface.co/sdadas/polish-splade\n",
    "- https://huggingface.co/facebook/xlm-v-base\n",
    "- https://huggingface.co/joelniklaus/legal-xlm-longformer-base\n",
    "- https://huggingface.co/EuropeanParliament/EUBERT\n",
    "- https://huggingface.co/OrlikB/KartonBERT_base_uncased_v1\n",
    "\n",
    "Ich odpowiedzi są załączone na końcu notatnika, nie zostały uwzględnione przy interpretacji wyników."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wyniki\n",
    "\n",
    "Ponieżej znajdują się wyniki dla utworzonych zapytań każdego z modeli, ale zanim przejdziemy do omawiania wyników każdego z osobna, przedstawie w prostej skali punktowej ich efektywność w każdej z kategorii. Punkty przyznaję w następujący sposób:\n",
    "- za każdy przypadek 1p. gdy co najmniej trzy odpowiedzi pasują do zadanego przypaku,\n",
    "- za każdą relację long-range 1p, jeśli wśród odpowiedzi znajduje się ta poprawna,\n",
    "- za każdą poprawną odpowiedź na pytanie z wiedzy ogólnej 1p. Wystarczy że odpowiedź ta będzie wśród TOP5.\n",
    "- za każde poprawnie ocenione nacechowanie 1p. Znów wystarczy, że zdatna odpowiedź znajduje się w TOP5.\n",
    "\n",
    "|            | bert-base-polish-cased-v1 | trelbert | xlm-roberta-base  | bernice |\n",
    "|------------|---------------------------|----------|-------------------|---------|\n",
    "| Przypadki  | 7/7                       | 7/7      | 6/7               | 5/7     |\n",
    "| Long-range | 3/3                       | 3/3      | 3/3               | 2/3     |\n",
    "| Wiedza     | 2/4                       | 1/4      | 1/4               | 0/4     |\n",
    "| Sentyment  | 4/5                       | 4/5      | 2/5               | 1/5     |\n",
    "\n",
    "Patrząc na sumaryczne wyniki najlepiej wypadł `bert-base`, wyprzedzając jedynie o 1p. `trelbert`. Model `xlm-roberta` ustępuje pierwszej dwójce a `bernice` pozostaje daleko w tyle, chociaż jest lepsza od więkoszości odrzuconych modeli.\n",
    "\n",
    "Wszystkie modele poradziły sobie z przypadkami bardzo dobrze. `bert-base` i `trelbert` były bezbłędne, `xlm-roberta` nie poradziła sobie wystarczająco z wołaczem, a `bernice` poza brakami w wołaczu, poległa również na celowniku.\n",
    "\n",
    "Jeśli chodzi o zachowanie relacji long-range dla podmiotu, to właściwie wszystkie modele poradziły sobie bardzo dobrze, jedynie `bernice` nie popełniła błąd, reszta była bezbłędna.\n",
    "\n",
    "W przypadku wiedzy o świecie, to tutaj nasze podstawowe modele nie wypadły najlepiej. `bert-base` dał dobrą odpowiedź na połowę pytań, `trelbert` i `xlm-roberta` tylko na jedno, a `bernice` nie dała żadnej prawidłowej odpowiedzi.\n",
    "\n",
    "Klasyfikacja zero-shot dla sentymentu przebiegła dla `bert-base` i `trelbert` bardzo dobrze, gdyż właściwa odpowiedź nie została uwzględniona w zbiorze odpowiedzi tylko raz. `xlm-roberta` zawarła w zbiorze odpowiedzi dwie poprawne odpowiedzi, a `bernice` tylko jedną. Dodam tylko, że przy ocenie klasyfikacji sentymentu byłem dość pobłażliwy dla wszystkich modeli.\n",
    "\n",
    "Więcej na temat każdej z ocenianych kategorii, jak i nabardziej jaskrawych błędów można przeczytać w sekcji dotyczącej każdego modelu z osobna."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bert-base-polish-cased-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PRZYPADKI: \"Ta >>><<< jest bardzo mocno napompowana.\" <- ['woda', 'rzeka', 'ziemia', 'energia', 'linia']\n",
      "PRZYPADKI: \"Naucz się >>><<< na jutrzejszy sprawdzian.\" <- ['tego', 'chodzić', 'angielskiego', 'liczyć', 'czegoś']\n",
      "PRZYPADKI: \"Prawdziwemu >>><<< zawsze można ufać.\" <- ['człowiekowi', 'nie', 'Bogu', 'królowi', 'przyjacielowi']\n",
      "PRZYPADKI: \"W deszczowy dzień zabierz ze sobą >>><<<.\" <- ['dzieci', 'psa', 'konie', 'ludzi', 'dziecko']\n",
      "PRZYPADKI: \"Uwielbiam spędzać czas z >>><<<.\" <- ['dziećmi', 'tobą', 'przyjaciółmi', 'rodziną', 'ludźmi']\n",
      "PRZYPADKI: \"Opowiadanie o >>><<< zawsze doprowadza mnie do łez.\" <- ['tym', 'nim', 'niej', 'sobie', 'tobie']\n",
      "PRZYPADKI: \"Szanowna >>><<<, nawiązując do otrzymanych zastrzeżeń...\" <- ['Komisjo', 'pani', 'Izbo', 'Pani', 'Komisja']\n",
      "\n",
      "RELACJA: \"Gdy odbierasz paczkę, sprawdź >>><<< jeszcze przy kurierze.\" <- ['to', 'ją', 'co', 'mnie', 'coś']\n",
      "RELACJA: \"Mijając ludzi na ulicy nie przyglądaj się >>><<< zbyt natarczywie.\" <- ['im', 'temu', 'jej', 'mu', 'ich']\n",
      "RELACJA: \"Gdy już go znajdziesz, zrób z >>><<< co uważasz za stosowne.\" <- ['nim', 'tym', 'nią', 'nimi', 'nami']\n",
      "\n",
      "WIEDZA: \"Stolicą Niemiec jest >>><<<.\" <- ['Polska', 'Warszawa', 'Berlin', 'Szwajcaria', 'Szwecja']\n",
      "WIEDZA: \"Ziemia jest >>><<< planetą od Słońca.\" <- ['większą', 'inną', 'najbliższą', 'mniejszą', 'większa']\n",
      "WIEDZA: \">>><<< jest procesem zmiany ze stanu ciekłego w stan gazowy.\" <- ['Proces', 'Gaz', 'Zjawisko', 'Energia', 'Woda']\n",
      "WIEDZA: \"Wisła uchodzi do >>><<<.\" <- ['morza', 'Wisły', 'Bałtyku', 'Odry', 'Warty']\n",
      "\n",
      "NACECHOWANIE: \"'Jestem w niej zakochany na zabój'  jest zdaniem wyrażającym >>><<<.\" <- ['miłość', 'uczucia', 'emocje', 'nienawiść', 'współczucie']\n",
      "NACECHOWANIE: \"'Motyla noga, nigdy nie znajdę już tego świerszcza' powiedziała osoba o nastawieniu >>><<<.\" <- ['człowieka', 'religijnym', 'ludzkim', 'kobiety', 'duchowym']\n",
      "NACECHOWANIE: \"'Jak ja nie cierpię tych okropnych Smerfów' to zdanie przedstawiające >>><<< emocje.\" <- ['moje', 'prawdziwe', 'ludzkie', 'złe', 'pozytywne']\n",
      "NACECHOWANIE: \"'Hura, nareszcie zdałem do kolejnej klasy' powiedziała osoba czująca się >>><<<.\" <- ['dobrze', 'samotna', 'lepiej', 'winna', 'źle']\n",
      "NACECHOWANIE: \"'Niech on tylko wpadnie w moje ręce' spośród emocji pozytywna, neutralna i negatywna, najbliżej temu zdaniu jest emocja >>><<<.\" <- ['negatywna', 'pozytywna', 'przeciwna', \"'\", 'właściwa']\n"
     ]
    }
   ],
   "source": [
    "#https://huggingface.co/dkleczek/bert-base-polish-cased-v1\n",
    "evaluate_lm(examples, 'dkleczek/bert-base-polish-cased-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przypadki wręcz wzorowo, jedynie w bierniku zbudowane zdanie nie miałoby dla nas sensu, ale gramatycznie jest poprawne.\n",
    "\n",
    "Przy relacjach long-range, dla pierwszego zdania optymalna odpowiedź znajduje się na drugim miejscu, dla pozostałych pierwsza odpowiedź jest zarazem tą najlepszą.\n",
    "\n",
    "Wiedza nie jest mocną stroną tego modelu, chociaż i tak wypadł najlepiej z badanych. Dwa pytania nie uzyskały poprawnej odpowiedzi, a dla dwóch pozostałych poprawna odpowiedź była dopiero na trzecim miejscu wśród propozycji. Mimo tego, poprawna odpowiedź o uchodzenie Wisły do Bałtyku zrobiła na mnie bardzo dobre wrażenie.\n",
    "\n",
    "Co do klasyfikacji sentymentu, to nie upierałem się przy zdefiniowanych uprzednio klasach, a chciałem zobaczyć wolne podejście modelu. W pierwszym zdaniu `nienawiść` zdecydowanie nie pasuje do reszty, drugie okazało się zbyt trudne do sklasyfikowania, trzecie zawiera odpowiedź klasyfikującą zdanie negatywnie na czwartej pozycji, lecz odpowiedź numer 5 uważa odwrotnie. Zdanie czwartek ma dobre odpowiedzi na pozycjach 1 i 3, natomiast pozostałe tu nie pasują, a zdanie piąte otrzymało poprawną odpowiedź, czyli `negatywna` na pozycji 1, ale `pozytywna` była zaraz za nią. Przynajmniej model skorzystał z sugerowanych klas sentymentu, z czym pozostałe modele miały problem.\n",
    "\n",
    "Ewidentnie nie widać spójności w klasyfikacji sentymentu. Skrajnie przeciwne klasy wyceniane są w zbiorze odpowiedzi jedna za drugą. Jest to największa bolączka tego modelu na równi z jednak dość ograniczoną wiedzą o świecie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trelbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PRZYPADKI: \"Ta >>><<< jest bardzo mocno napompowana.\" <- ['pani', 'kobieta', 'piłka', 'strona', 'Pani']\n",
      "PRZYPADKI: \"Naucz się >>><<< na jutrzejszy sprawdzian.\" <- ['polskiego', 'angielskiego', 'hiszpańskiego', 'pisać', 'niemieckiego']\n",
      "PRZYPADKI: \"Prawdziwemu >>><<< zawsze można ufać.\" <- ['człowiekowi', 'Bogu', 'Panu', 'prezydentowi', 'panu']\n",
      "PRZYPADKI: \"W deszczowy dzień zabierz ze sobą >>><<<.\" <- ['rower', 'psa', 'plecak', 'czapkę', 'kota']\n",
      "PRZYPADKI: \"Uwielbiam spędzać czas z >>><<<.\" <- ['rodziną', 'ludźmi', 'dziećmi', 'przyjaciółmi', 'psem']\n",
      "PRZYPADKI: \"Opowiadanie o >>><<< zawsze doprowadza mnie do łez.\" <- ['tym', 'nim', 'nich', 'sobie', 'tobie']\n",
      "PRZYPADKI: \"Szanowna >>><<<, nawiązując do otrzymanych zastrzeżeń...\" <- ['Pani', 'pani', '@anonymized_account', 'Państwo', 'Rada']\n",
      "\n",
      "RELACJA: \"Gdy odbierasz paczkę, sprawdź >>><<< jeszcze przy kurierze.\" <- ['ją', 'się', 'to', 'czy', 'go']\n",
      "RELACJA: \"Mijając ludzi na ulicy nie przyglądaj się >>><<< zbyt natarczywie.\" <- ['im', 'sobie', 'temu', 'innym', 'ludziom']\n",
      "RELACJA: \"Gdy już go znajdziesz, zrób z >>><<< co uważasz za stosowne.\" <- ['nim', 'tym', 'nimi', 'nią', 'sobą']\n",
      "\n",
      "WIEDZA: \"Stolicą Niemiec jest >>><<<.\" <- ['Berlin', 'Monachium', 'Warszawa', 'Wrocław', 'Gdańsk']\n",
      "WIEDZA: \"Ziemia jest >>><<< planetą od Słońca.\" <- ['inną', 'większą', 'mniejszą', 'drugą', 'wyższą']\n",
      "WIEDZA: \">>><<< jest procesem zmiany ze stanu ciekłego w stan gazowy.\" <- ['To', 'Gaz', 'Proces', 'Woda', 'Życie']\n",
      "WIEDZA: \"Wisła uchodzi do >>><<<.\" <- ['Wisły', 'Unii', 'UE', 'ekstraklasy', 'Ekstraklasy']\n",
      "\n",
      "NACECHOWANIE: \"'Jestem w niej zakochany na zabój'  jest zdaniem wyrażającym >>><<<.\" <- ['miłość', 'emocje', 'uczucia', 'nienawiść', 'smutek']\n",
      "NACECHOWANIE: \"'Motyla noga, nigdy nie znajdę już tego świerszcza' powiedziała osoba o nastawieniu >>><<<.\" <- ['religijnym', 'pozytywnym', 'politycznym', 'ludzkim', 'sportowym']\n",
      "NACECHOWANIE: \"'Jak ja nie cierpię tych okropnych Smerfów' to zdanie przedstawiające >>><<< emocje.\" <- ['moje', 'prawdziwe', 'te', 'najgorsze', 'wszystkie']\n",
      "NACECHOWANIE: \"'Hura, nareszcie zdałem do kolejnej klasy' powiedziała osoba czująca się >>><<<.\" <- ['zagrożona', 'winna', 'bezpiecznie', 'dobrze', 'świetnie']\n",
      "NACECHOWANIE: \"'Niech on tylko wpadnie w moje ręce' spośród emocji pozytywna, neutralna i negatywna, najbliżej temu zdaniu jest emocja >>><<<.\" <- ['nienawiści', 'pozytywna', 'miłości', 'strachu', 'zimna']\n"
     ]
    }
   ],
   "source": [
    "#https://huggingface.co/deepsense-ai/trelbert\n",
    "evaluate_lm(examples, 'deepsense-ai/trelbert')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przypadki jak w przypadku `bert-base` świetnie, jedynie zdania w bierniku nie mają sensu. Poza tym nie wszystkie odpowiedzi dla wołacza są odpowiednie, ale za to propozycje dla dopełniacza wyglądają najlepiej wśród wszystkich badanych modeli.\n",
    "\n",
    "Relacje long-range bezbłędnie z poprawnymi odpowiedziami na pierwszym miejscu.\n",
    "\n",
    "Z wiedzy 5 za geografię i podanie Berlina jako pierwszej odpowiedzi na pytanie o stolicę Niemiec. Następne pytania dość kiepsko, a przy ostatnim o Wisłę, model zapewnie pomylił rzekę z klubem piłkarskim.\n",
    "\n",
    "Co do badania sentymentu, to widać pewne zbierzności z `base-bert`. W pierwszym również zdecydowanie nie pasuje `nienawiść`, najpewnie modele dały się złapać przez dość przekorne sformułowanie \"na zabój\". Drugie jest kompletnie źle. Trzecie z odpowiedzią z czwartego miejsca `najgorsze`, zostało poprawnie sklasyfikowane negatywnie, pozostałe odpowiedzi są akceptowalne, ale słabo klasyfikują sentyment. Dla zdania czwartego, odpowiedzi od trzeciej są poprawne, pierwsza i druga są poniekąd ich przeciwieństwem i nie pasują po wypowiedzianego zdania. Zdanie piąte dostało poprawną odpowidź `nienawiści` na miejscu pierwszym. Mankamentem jest nie skorzystanie z podpowiedzianych klas oraz usytuowaniem odpowiedzi `pozytywna` oraz `miłości` zaraz za `nienawiści`. Czyżby i dla modeli językowych granica między miłością a nienawiścią również była tak cieńka?!? XD\n",
    "\n",
    "Generalnie, model ma podobne przypadłości co `base-bert`, przeplata opinie pozytywne z negatywnymi i mało wie o świecie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### xlm-roberta-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PRZYPADKI: \"Ta >>><<< jest bardzo mocno napompowana.\" <- ['część', 'cena', 'strona', 'farba', 'maska']\n",
      "PRZYPADKI: \"Naucz się >>><<< na jutrzejszy sprawdzian.\" <- ['przygotować', 'wszystkiego', 'odpowiedzi', 'angielskiego', 'już']\n",
      "PRZYPADKI: \"Prawdziwemu >>><<< zawsze można ufać.\" <- ['Bogu', 'nie', 'życiu', 'dziecku', ',']\n",
      "PRZYPADKI: \"W deszczowy dzień zabierz ze sobą >>><<<.\" <- ['wodę', 'śnieg', 'rower', 'kwiaty', 'słońce']\n",
      "PRZYPADKI: \"Uwielbiam spędzać czas z >>><<<.\" <- ['dziećmi', 'dzieckiem', 'ludźmi', 'innymi', 'nimi']\n",
      "PRZYPADKI: \"Opowiadanie o >>><<< zawsze doprowadza mnie do łez.\" <- ['miłości', 'sobie', 'życiu', 'przeszłości', 'niej']\n",
      "PRZYPADKI: \"Szanowna >>><<<, nawiązując do otrzymanych zastrzeżeń...\" <- ['Pani', 'Państwo', 'pani', 'Państwa', 'strona']\n",
      "\n",
      "RELACJA: \"Gdy odbierasz paczkę, sprawdź >>><<< jeszcze przy kurierze.\" <- ['ją', 'to', 'czy', 'je', 'sytuację']\n",
      "RELACJA: \"Mijając ludzi na ulicy nie przyglądaj się >>><<< zbyt natarczywie.\" <- ['im', 'nim', 'tym', 'sobie', 'jim']\n",
      "RELACJA: \"Gdy już go znajdziesz, zrób z >>><<< co uważasz za stosowne.\" <- ['nim', 'tym', 'tego', 'niego', 'czymś']\n",
      "\n",
      "WIEDZA: \"Stolicą Niemiec jest >>><<<.\" <- ['Polska', 'Warszawa', 'Berlin', 'Bayern', 'kraj']\n",
      "WIEDZA: \"Ziemia jest >>><<< planetą od Słońca.\" <- ['większą', 'drugą', 'dalej', 'jedną', 'większa']\n",
      "WIEDZA: \">>><<< jest procesem zmiany ze stanu ciekłego w stan gazowy.\" <- ['Proces', 'Energia', 'Gaz', 'LPG', 'Wszystko']\n",
      "WIEDZA: \"Wisła uchodzi do >>><<<.\" <- ['formy', 'góry', 'stanu', 'użytku', 'wody']\n",
      "\n",
      "NACECHOWANIE: \"'Jestem w niej zakochany na zabój'  jest zdaniem wyrażającym >>><<<.\" <- ['uczucia', 'siebie', 'miłość', 'się', 'związek']\n",
      "NACECHOWANIE: \"'Motyla noga, nigdy nie znajdę już tego świerszcza' powiedziała osoba o nastawieniu >>><<<.\" <- ['.', 'B', 'kobiety', 'człowieka', 'mężczyzn']\n",
      "NACECHOWANIE: \"'Jak ja nie cierpię tych okropnych Smerfów' to zdanie przedstawiające >>><<< emocje.\" <- ['moje', 'nasze', 'prawdziwe', 'jego', 'jej']\n",
      "NACECHOWANIE: \"'Hura, nareszcie zdałem do kolejnej klasy' powiedziała osoba czująca się >>><<<.\" <- ['dobrze', 'bezpiecznie', 'wyjątkowo', 'świetnie', 'doskonale']\n",
      "NACECHOWANIE: \"'Niech on tylko wpadnie w moje ręce' spośród emocji pozytywna, neutralna i negatywna, najbliżej temu zdaniu jest emocja >>><<<.\" <- ['negativa', 'pozytywne', 'positiva', 'miłości', '.']\n"
     ]
    }
   ],
   "source": [
    "#https://huggingface.co/FacebookAI/xlm-roberta-base\n",
    "evaluate_lm(examples, 'xlm-roberta-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przypadki bardzo dobrze, jak zwykle sugestie w bierniku nie mają sensu, aczkolwiek są gramatycznie poprawne. W wołaczu tylko dwie sugestie są poprawne, stąd brak punktu.\n",
    "\n",
    "Relacja long-range poprawnie już przy pierwszych odpowiedziach.\n",
    "\n",
    "Model odpowiedział poprawnie tylko na pytanie o stolicę Niemiec, ale nie była ona na miejscu pierwszym. Pozostałe odpowiedzi są błędne.\n",
    "\n",
    "Dla pierwszego zdania wyrażającego emocje w zasadzie wszystkie zaproponowane odpowiedzi są poprawne. Drugie nie ma poprawnych sugestii, a co gorsze, model sugeruje drugą kropkę na końcu zdania. Dla trzeciego zdania brak klasyfikacji emocji. Czwarte ma wszytkie odpowiedzi poprawne. Piąte miesza języki, faktycznie proponuje poprawną odpowiedź na pierwszym miejscu, lecz nie po polsku, a zaraz za nią, stawia dwukrotnie odpoiwdź przeciwną, gdzie również miesza języki. Widać staranie użycia zaproponowanych klas.\n",
    "\n",
    "Model ten wie o świecie jeszcze mniej, ale za to rzadziej przypisuje skrajnie przeciwne klasy sentymentu tym samym zdaniom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at jhu-clsp/bernice were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PRZYPADKI: \"Ta >>><<< jest bardzo mocno napompowana.\" <- ['kobieta', 'pani', 'Pani', 'osoba', 'lista']\n",
      "PRZYPADKI: \"Naucz się >>><<< na jutrzejszy sprawdzian.\" <- ['matmy', 'pisać', 'angielskiego', 'iść', 'napisać']\n",
      "PRZYPADKI: \"Prawdziwemu >>><<< zawsze można ufać.\" <- ['nie', ',', 'Bogu', 'ludziom', 'człowieka']\n",
      "PRZYPADKI: \"W deszczowy dzień zabierz ze sobą >>><<<.\" <- ['mnie', 'siebie', 'wszystko', 'jedzenie', 'życie']\n",
      "PRZYPADKI: \"Uwielbiam spędzać czas z >>><<<.\" <- ['nią', 'nim', 'nimi', 'ludźmi', 'kim']\n",
      "PRZYPADKI: \"Opowiadanie o >>><<< zawsze doprowadza mnie do łez.\" <- ['sobie', 'miłości', 'Tobie', 'tym', 'wszystkim']\n",
      "PRZYPADKI: \"Szanowna >>><<<, nawiązując do otrzymanych zastrzeżeń...\" <- ['Pani', 'pani', 'odpowiedź', 'osoba', 'opozycja']\n",
      "\n",
      "RELACJA: \"Gdy odbierasz paczkę, sprawdź >>><<< jeszcze przy kurierze.\" <- ['czy', 'się', 'to', 'cie', 'je']\n",
      "RELACJA: \"Mijając ludzi na ulicy nie przyglądaj się >>><<< zbyt natarczywie.\" <- ['im', 'na', 'tam', 'ich', ',']\n",
      "RELACJA: \"Gdy już go znajdziesz, zrób z >>><<< co uważasz za stosowne.\" <- ['tego', 'tym', 'niego', 'siebie', 'nim']\n",
      "\n",
      "WIEDZA: \"Stolicą Niemiec jest >>><<<.\" <- ['Polska', 'Putin', 'Tusk', 'Kaczyński', 'Morawiecki']\n",
      "WIEDZA: \"Ziemia jest >>><<< planetą od Słońca.\" <- ['inna', 'bardziej', 'jedną', 'druga', 'lepsze']\n",
      "WIEDZA: \">>><<< jest procesem zmiany ze stanu ciekłego w stan gazowy.\" <- ['To', 'Co', 'Nie', 'Brexit', 'Tak']\n",
      "WIEDZA: \"Wisła uchodzi do >>><<<.\" <- ['domu', 'końca', 'gry', 'klubu', 'roboty']\n",
      "\n",
      "NACECHOWANIE: \"'Jestem w niej zakochany na zabój'  jest zdaniem wyrażającym >>><<<.\" <- ['mnie', 'się', 'strach', 'opinie', 'wrażenie']\n",
      "NACECHOWANIE: \"'Motyla noga, nigdy nie znajdę już tego świerszcza' powiedziała osoba o nastawieniu >>><<<.\" <- ['.', 'się', 'psa', 'Tuska', 'kościoła']\n",
      "NACECHOWANIE: \"'Jak ja nie cierpię tych okropnych Smerfów' to zdanie przedstawiające >>><<< emocje.\" <- ['moje', 'prawdziwe', 'swoje', 'pewne', 'takie']\n",
      "NACECHOWANIE: \"'Hura, nareszcie zdałem do kolejnej klasy' powiedziała osoba czująca się >>><<<.\" <- ['źle', 'dobrze', 'lepiej', 'sobą', 'zle']\n",
      "NACECHOWANIE: \"'Niech on tylko wpadnie w moje ręce' spośród emocji pozytywna, neutralna i negatywna, najbliżej temu zdaniu jest emocja >>><<<.\" <- ['.', '...', ',', '-', ':)']\n"
     ]
    }
   ],
   "source": [
    "#https://huggingface.co/jhu-clsp/bernice\n",
    "evaluate_lm(examples, 'jhu-clsp/bernice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model poradził sobie z przypadkami najgorzej, lecz wciąż przyzwoicie. W celowniku tylko jedna odpowiedź jest poprawna, a w wołaczu dwie, a więc brakuje trochę do przyznania punktu. Odpowiedzi dla biernika, jak i we wszystkich powyższych modelach, są poprawne lecz nie stanowią dobrej sugestii dla czlowieka.\n",
    "\n",
    "Relacja long-range zachowana dla dwóch z trzech przypadków, ale tylko dla drugiego zdania optymalna odpowiedź była pierwszą sugestią.\n",
    "\n",
    "Wiedza modelu to totalna kompromitacja. 2.0, do poprawki!\n",
    "\n",
    "Sentyment został również poprawnie sklasyfikowany w jednym przypadku, a mianowicie w zdaniu czwartym, aczkolwiek i tak jest to naciągany punkt, gdyż wśród odpowiedzi, nacechowanie pozytywne do negatywnego wystąpuje w stosunku 2:2. Pozostałe klasyfikacji nie mają kompletnie sensu, model wstawia wielokrotne znaki interpunkcji i nie uwzględnia podpowiedzi.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dodatkowe uwagi\n",
    "\n",
    "- Zakończenie zdania kropką zniechęca model od wypełnienia maski znakiem interpunkcyjnym.\n",
    "- Podanie możliwych odpowiedzi dla klasyfikacji sentymentu zostało wykorzystane w dwóch z czterech przypadków."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modele językowe wypróbowane, lecz nie zestawione w opracowaniu wyników"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PRZYPADKI: \"Ta >>><<< jest bardzo mocno napompowana.\" <- ['wersja', 'piosenka', 'droga', 'płyta', 'ostatnia']\n",
      "PRZYPADKI: \"Naucz się >>><<< na jutrzejszy sprawdzian.\" <- ['tez', 'takze', 'to', 'rowniez', 'go']\n",
      "PRZYPADKI: \"Prawdziwemu >>><<< zawsze można ufać.\" <- ['nie', ',', 'prawie', 'niemal', '-']\n",
      "PRZYPADKI: \"W deszczowy dzień zabierz ze sobą >>><<<.\" <- ['dzieci', 'ludzi', 'ok', 'kobiety', 'wiecej']\n",
      "PRZYPADKI: \"Uwielbiam spędzać czas z >>><<<.\" <- ['nim', 'nia', 'matka', 'rodzina', 'muzyka']\n",
      "PRZYPADKI: \"Opowiadanie o >>><<< zawsze doprowadza mnie do łez.\" <- ['tym', 'nim', 'sobie', 'zyciu', 'niej']\n",
      "PRZYPADKI: \"Szanowna >>><<<, nawiązując do otrzymanych zastrzeżeń...\" <- ['\"', '.', ')', '##ch', '##mi']\n",
      "\n",
      "RELACJA: \"Gdy odbierasz paczkę, sprawdź >>><<< jeszcze przy kurierze.\" <- ['##y', '##ono', '##ons', '##my', '##one']\n",
      "RELACJA: \"Mijając ludzi na ulicy nie przyglądaj się >>><<< zbyt natarczywie.\" <- ['mu', 'im', 'na', 'sobie', 'juz']\n",
      "RELACJA: \"Gdy już go znajdziesz, zrób z >>><<< co uważasz za stosowne.\" <- [',', 'tego', '##yje', 'tym', '##y']\n",
      "\n",
      "WIEDZA: \"Stolicą Niemiec jest >>><<<.\" <- ['berlin', 'bonn', 'erfurt', 'karlsruhe', 'hamburg']\n",
      "WIEDZA: \"Ziemia jest >>><<< planetą od Słońca.\" <- ['inna', 'wieksza', 'najwieksza', 'druga', 'trzecia']\n",
      "WIEDZA: \">>><<< jest procesem zmiany ze stanu ciekłego w stan gazowy.\" <- ['proces', 'temperatura', 'energia', 'to', 'faza']\n",
      "WIEDZA: \"Wisła uchodzi do >>><<<.\" <- ['tzw', 'woj', 'sw', 'r', 'polski']\n",
      "\n",
      "NACECHOWANIE: \"'Jestem w niej zakochany na zabój'  jest zdaniem wyrażającym >>><<<.\" <- ['sie', 'to', 'autora', 'słowa', 'np']\n",
      "NACECHOWANIE: \"'Motyla noga, nigdy nie znajdę już tego świerszcza' powiedziała osoba o nastawieniu >>><<<.\" <- ['sie', 'zespołu', 'tekstu', 'autora', 'słowa']\n",
      "NACECHOWANIE: \"'Jak ja nie cierpię tych okropnych Smerfów' to zdanie przedstawiające >>><<< emocje.\" <- ['jej', 'jego', 'te', 'swoje', 'ich']\n",
      "NACECHOWANIE: \"'Hura, nareszcie zdałem do kolejnej klasy' powiedziała osoba czująca się >>><<<.\" <- ['nia', 'nim', 'to', 'tym', 'tam']\n",
      "NACECHOWANIE: \"'Niech on tylko wpadnie w moje ręce' spośród emocji pozytywna, neutralna i negatywna, najbliżej temu zdaniu jest emocja >>><<<.\" <- ['dobra', 'inna', 'człowieka', 'np', 'obecna']\n"
     ]
    }
   ],
   "source": [
    "#https://huggingface.co/google-bert/bert-base-multilingual-uncased\n",
    "evaluate_lm(examples, 'bert-base-multilingual-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PRZYPADKI: \"Ta >>><<< jest bardzo mocno napompowana.\" <- ['sytuacja', 'szkoła', 'Polska', 'Ukraina', 'książka']\n",
      "PRZYPADKI: \"Naucz się >>><<< na jutrzejszy sprawdzian.\" <- ['już', 'jutro', 'dzisiaj', 'dziś', 'teraz']\n",
      "PRZYPADKI: \"Prawdziwemu >>><<< zawsze można ufać.\" <- ['nie', 'się', ',', 'to', 'na']\n",
      "PRZYPADKI: \"W deszczowy dzień zabierz ze sobą >>><<<.\" <- ['wodę', 'Polskę', 'kwiaty', 'miłość', 'siebie']\n",
      "PRZYPADKI: \"Uwielbiam spędzać czas z >>><<<.\" <- ['ludźmi', 'innymi', 'nimi', 'nim', 'sobą']\n",
      "PRZYPADKI: \"Opowiadanie o >>><<< zawsze doprowadza mnie do łez.\" <- ['mnie', 'pracę', 'sobie', 'pracy', 'tym']\n",
      "PRZYPADKI: \"Szanowna >>><<<, nawiązując do otrzymanych zastrzeżeń...\" <- ['praca', 'jestem', 'miłość', 'rzecz', 'Polska']\n",
      "\n",
      "RELACJA: \"Gdy odbierasz paczkę, sprawdź >>><<< jeszcze przy kurierze.\" <- ['się', 'ją', 'je', 'to', 'sie']\n",
      "RELACJA: \"Mijając ludzi na ulicy nie przyglądaj się >>><<< zbyt natarczywie.\" <- ['aż', 'nawet', 'im', 'jakoś', ',']\n",
      "RELACJA: \"Gdy już go znajdziesz, zrób z >>><<< co uważasz za stosowne.\" <- ['tego', 'tym', 'niego', 'nim', 'siebie']\n",
      "\n",
      "WIEDZA: \"Stolicą Niemiec jest >>><<<.\" <- ['Polska', 'Putin', 'Merkel', 'UE', 'Ukraina']\n",
      "WIEDZA: \"Ziemia jest >>><<< planetą od Słońca.\" <- ['większą', 'większa', 'drugą', 'pierwszą', 'jedną']\n",
      "WIEDZA: \">>><<< jest procesem zmiany ze stanu ciekłego w stan gazowy.\" <- ['Polska', 'Putin', 'Pegasus', 'Ukraina', 'UE']\n",
      "WIEDZA: \"Wisła uchodzi do >>><<<.\" <- ['Polski', 'Legii', 'domu', 'gry', 'Warszawy']\n",
      "\n",
      "NACECHOWANIE: \"'Jestem w niej zakochany na zabój'  jest zdaniem wyrażającym >>><<<.\" <- ['się', 'rozum', 'słowa', 'prawo', 'człowieka']\n",
      "NACECHOWANIE: \"'Motyla noga, nigdy nie znajdę już tego świerszcza' powiedziała osoba o nastawieniu >>><<<.\" <- ['krwi', 'psa', 'dzieci', 'włosów', 'kota']\n",
      "NACECHOWANIE: \"'Jak ja nie cierpię tych okropnych Smerfów' to zdanie przedstawiające >>><<< emocje.\" <- ['ogromne', 'moje', 'prawdziwe', 'swoje', 'różne']\n",
      "NACECHOWANIE: \"'Hura, nareszcie zdałem do kolejnej klasy' powiedziała osoba czująca się >>><<<.\" <- ['dobrze', 'źle', 'lepiej', 'teraz', 'inaczej']\n",
      "NACECHOWANIE: \"'Niech on tylko wpadnie w moje ręce' spośród emocji pozytywna, neutralna i negatywna, najbliżej temu zdaniu jest emocja >>><<<.\" <- ['.', '...', 'raka', '!', '...']\n"
     ]
    }
   ],
   "source": [
    "#https://huggingface.co/Twitter/twhin-bert-base\n",
    "evaluate_lm(examples, 'Twitter/twhin-bert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PRZYPADKI: \"Ta >>><<< jest bardzo mocno napompowana.\" <- ['♠', '₁', '♣', '̄', 'rima']\n",
      "PRZYPADKI: \"Naucz się >>><<< na jutrzejszy sprawdzian.\" <- ['kończy', 'zmienia', 'gra', '↘', 'dało']\n",
      "PRZYPADKI: \"Prawdziwemu >>><<< zawsze można ufać.\" <- ['↘', '⑩', '⑤', '█', '④']\n",
      "PRZYPADKI: \"W deszczowy dzień zabierz ze sobą >>><<<.\" <- ['rzadko', 'itd', 'wiosną', 'będzie', '2-2']\n",
      "PRZYPADKI: \"Uwielbiam spędzać czas z >>><<<.\" <- ['powrotem', 'wodą', 'przyszłości', 'kością', 'każdym']\n",
      "PRZYPADKI: \"Opowiadanie o >>><<< zawsze doprowadza mnie do łez.\" <- ['mnie', 'czym', '♣', 'miłości', 'drugą']\n",
      "PRZYPADKI: \"Szanowna >>><<<, nawiązując do otrzymanych zastrzeżeń...\" <- ['↘', '►', '➜', '◄', '↓']\n",
      "\n",
      "RELACJA: \"Gdy odbierasz paczkę, sprawdź >>><<< jeszcze przy kurierze.\" <- ['esz', 'się', 'taką', 'gum', 'głowę']\n",
      "RELACJA: \"Mijając ludzi na ulicy nie przyglądaj się >>><<< zbyt natarczywie.\" <- ['niczym', 'dając', 'poprzez', 'żeby', 'odpowiada']\n",
      "RELACJA: \"Gdy już go znajdziesz, zrób z >>><<< co uważasz za stosowne.\" <- ['płu', 'dziec', 'tym', 'któr', 'prawdo']\n",
      "\n",
      "WIEDZA: \"Stolicą Niemiec jest >>><<<.\" <- ['używana', 'używany', 'obecny', 'niem.', 'mowa']\n",
      "WIEDZA: \"Ziemia jest >>><<< planetą od Słońca.\" <- ['średnią', 'małą', 'największą', 'wielką', 'kolejną']\n",
      "WIEDZA: \">>><<< jest procesem zmiany ze stanu ciekłego w stan gazowy.\" <- ['►', '◄', '٭', '☰', '⌋']\n",
      "WIEDZA: \"Wisła uchodzi do >>><<<.\" <- ['dziś', 'Wisła', 'Wisły', 'Włoch', 'Lwowa']\n",
      "\n",
      "NACECHOWANIE: \"'Jestem w niej zakochany na zabój'  jest zdaniem wyrażającym >>><<<.\" <- ['prawym', 'liczbą', \"'\", 'zdaniem', 'wolnym']\n",
      "NACECHOWANIE: \"'Motyla noga, nigdy nie znajdę już tego świerszcza' powiedziała osoba o nastawieniu >>><<<.\" <- ['B2', 'naturalnych', 'Na', 'wolnym', 'podstawowym']\n",
      "NACECHOWANIE: \"'Jak ja nie cierpię tych okropnych Smerfów' to zdanie przedstawiające >>><<< emocje.\" <- ['silne', 'różne', 'stany', 'następujące', 'własne']\n",
      "NACECHOWANIE: \"'Hura, nareszcie zdałem do kolejnej klasy' powiedziała osoba czująca się >>><<<.\" <- ['numerem', 'dalej', 'inaczej', 'wyżej', '沽']\n",
      "NACECHOWANIE: \"'Niech on tylko wpadnie w moje ręce' spośród emocji pozytywna, neutralna i negatywna, najbliżej temu zdaniu jest emocja >>><<<.\" <- ['dobra', 'negativa', 'zła', 'pierwsza', 'mała']\n"
     ]
    }
   ],
   "source": [
    "#https://huggingface.co/FacebookAI/xlm-mlm-17-1280\n",
    "evaluate_lm(examples, 'xlm-mlm-17-1280')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at studio-ousia/mluke-base were not used when initializing LukeForMaskedLM: ['luke.encoder.layer.0.attention.self.e2e_query.bias', 'luke.encoder.layer.0.attention.self.e2e_query.weight', 'luke.encoder.layer.0.attention.self.e2w_query.bias', 'luke.encoder.layer.0.attention.self.e2w_query.weight', 'luke.encoder.layer.0.attention.self.w2e_query.bias', 'luke.encoder.layer.0.attention.self.w2e_query.weight', 'luke.encoder.layer.1.attention.self.e2e_query.bias', 'luke.encoder.layer.1.attention.self.e2e_query.weight', 'luke.encoder.layer.1.attention.self.e2w_query.bias', 'luke.encoder.layer.1.attention.self.e2w_query.weight', 'luke.encoder.layer.1.attention.self.w2e_query.bias', 'luke.encoder.layer.1.attention.self.w2e_query.weight', 'luke.encoder.layer.10.attention.self.e2e_query.bias', 'luke.encoder.layer.10.attention.self.e2e_query.weight', 'luke.encoder.layer.10.attention.self.e2w_query.bias', 'luke.encoder.layer.10.attention.self.e2w_query.weight', 'luke.encoder.layer.10.attention.self.w2e_query.bias', 'luke.encoder.layer.10.attention.self.w2e_query.weight', 'luke.encoder.layer.11.attention.self.e2e_query.bias', 'luke.encoder.layer.11.attention.self.e2e_query.weight', 'luke.encoder.layer.11.attention.self.e2w_query.bias', 'luke.encoder.layer.11.attention.self.e2w_query.weight', 'luke.encoder.layer.11.attention.self.w2e_query.bias', 'luke.encoder.layer.11.attention.self.w2e_query.weight', 'luke.encoder.layer.2.attention.self.e2e_query.bias', 'luke.encoder.layer.2.attention.self.e2e_query.weight', 'luke.encoder.layer.2.attention.self.e2w_query.bias', 'luke.encoder.layer.2.attention.self.e2w_query.weight', 'luke.encoder.layer.2.attention.self.w2e_query.bias', 'luke.encoder.layer.2.attention.self.w2e_query.weight', 'luke.encoder.layer.3.attention.self.e2e_query.bias', 'luke.encoder.layer.3.attention.self.e2e_query.weight', 'luke.encoder.layer.3.attention.self.e2w_query.bias', 'luke.encoder.layer.3.attention.self.e2w_query.weight', 'luke.encoder.layer.3.attention.self.w2e_query.bias', 'luke.encoder.layer.3.attention.self.w2e_query.weight', 'luke.encoder.layer.4.attention.self.e2e_query.bias', 'luke.encoder.layer.4.attention.self.e2e_query.weight', 'luke.encoder.layer.4.attention.self.e2w_query.bias', 'luke.encoder.layer.4.attention.self.e2w_query.weight', 'luke.encoder.layer.4.attention.self.w2e_query.bias', 'luke.encoder.layer.4.attention.self.w2e_query.weight', 'luke.encoder.layer.5.attention.self.e2e_query.bias', 'luke.encoder.layer.5.attention.self.e2e_query.weight', 'luke.encoder.layer.5.attention.self.e2w_query.bias', 'luke.encoder.layer.5.attention.self.e2w_query.weight', 'luke.encoder.layer.5.attention.self.w2e_query.bias', 'luke.encoder.layer.5.attention.self.w2e_query.weight', 'luke.encoder.layer.6.attention.self.e2e_query.bias', 'luke.encoder.layer.6.attention.self.e2e_query.weight', 'luke.encoder.layer.6.attention.self.e2w_query.bias', 'luke.encoder.layer.6.attention.self.e2w_query.weight', 'luke.encoder.layer.6.attention.self.w2e_query.bias', 'luke.encoder.layer.6.attention.self.w2e_query.weight', 'luke.encoder.layer.7.attention.self.e2e_query.bias', 'luke.encoder.layer.7.attention.self.e2e_query.weight', 'luke.encoder.layer.7.attention.self.e2w_query.bias', 'luke.encoder.layer.7.attention.self.e2w_query.weight', 'luke.encoder.layer.7.attention.self.w2e_query.bias', 'luke.encoder.layer.7.attention.self.w2e_query.weight', 'luke.encoder.layer.8.attention.self.e2e_query.bias', 'luke.encoder.layer.8.attention.self.e2e_query.weight', 'luke.encoder.layer.8.attention.self.e2w_query.bias', 'luke.encoder.layer.8.attention.self.e2w_query.weight', 'luke.encoder.layer.8.attention.self.w2e_query.bias', 'luke.encoder.layer.8.attention.self.w2e_query.weight', 'luke.encoder.layer.9.attention.self.e2e_query.bias', 'luke.encoder.layer.9.attention.self.e2e_query.weight', 'luke.encoder.layer.9.attention.self.e2w_query.bias', 'luke.encoder.layer.9.attention.self.e2w_query.weight', 'luke.encoder.layer.9.attention.self.w2e_query.bias', 'luke.encoder.layer.9.attention.self.w2e_query.weight']\n",
      "- This IS expected if you are initializing LukeForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LukeForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PRZYPADKI: \"Ta >>><<< jest bardzo mocno napompowana.\" <- ['pompa', 'bomba', 'część', 'droga', 'lampa']\n",
      "PRZYPADKI: \"Naucz się >>><<< na jutrzejszy sprawdzian.\" <- ['go', 'czasu', 'tego', 'mnie', 'gry']\n",
      "PRZYPADKI: \"Prawdziwemu >>><<< zawsze można ufać.\" <- ['nie', 'życiu', 'Bogu', 'prawie', 'niemal']\n",
      "PRZYPADKI: \"W deszczowy dzień zabierz ze sobą >>><<<.\" <- ['deszcz', 'wiatr', 'śnieg', ':', 'rok']\n",
      "PRZYPADKI: \"Uwielbiam spędzać czas z >>><<<.\" <- ['dziećmi', 'ludźmi', 'dzieckiem', 'wodą', 'sobą']\n",
      "PRZYPADKI: \"Opowiadanie o >>><<< zawsze doprowadza mnie do łez.\" <- ['miłości', 'tym', 'mnie', 'nim', 'niej']\n",
      "PRZYPADKI: \"Szanowna >>><<<, nawiązując do otrzymanych zastrzeżeń...\" <- ['strona', 'odpowiedź', 'decyzja', 'jest', ',']\n",
      "\n",
      "RELACJA: \"Gdy odbierasz paczkę, sprawdź >>><<< jeszcze przy kurierze.\" <- ['ją', 'się', 'to', 'mnie', 'jej']\n",
      "RELACJA: \"Mijając ludzi na ulicy nie przyglądaj się >>><<< zbyt natarczywie.\" <- ['im', 'tym', 'ich', 'nim', 'temu']\n",
      "RELACJA: \"Gdy już go znajdziesz, zrób z >>><<< co uważasz za stosowne.\" <- ['nim', 'tym', 'tego', 'kolei', 'niego']\n",
      "\n",
      "WIEDZA: \"Stolicą Niemiec jest >>><<<.\" <- ['miasto', ':', 'kraj', 'jednak', 'Berlin']\n",
      "WIEDZA: \"Ziemia jest >>><<< planetą od Słońca.\" <- ['większą', 'drugą', 'większa', 'jedną', 'dużą']\n",
      "WIEDZA: \">>><<< jest procesem zmiany ze stanu ciekłego w stan gazowy.\" <- ['Proces', 'Stan', 'Nie', 'Gas', 'Transport']\n",
      "WIEDZA: \"Wisła uchodzi do >>><<<.\" <- ['niego', 'źródeł', 'portu', 'wody', 'źródła']\n",
      "\n",
      "NACECHOWANIE: \"'Jestem w niej zakochany na zabój'  jest zdaniem wyrażającym >>><<<.\" <- [':', 'miłość', 'uczucie', 'uczucia', 'związek']\n",
      "NACECHOWANIE: \"'Motyla noga, nigdy nie znajdę już tego świerszcza' powiedziała osoba o nastawieniu >>><<<.\" <- ['.', ',', 'dziecka', 'na', ':']\n",
      "NACECHOWANIE: \"'Jak ja nie cierpię tych okropnych Smerfów' to zdanie przedstawiające >>><<< emocje.\" <- ['ich', 'moje', 'jego', 'te', 'swoje']\n",
      "NACECHOWANIE: \"'Hura, nareszcie zdałem do kolejnej klasy' powiedziała osoba czująca się >>><<<.\" <- ['dobrze', 'pewnie', 'źle', 'bezpiecznie', 'spokojnie']\n",
      "NACECHOWANIE: \"'Niech on tylko wpadnie w moje ręce' spośród emocji pozytywna, neutralna i negatywna, najbliżej temu zdaniu jest emocja >>><<<.\" <- ['pozytywne', ':', 'negativa', '.', 'taka']\n"
     ]
    }
   ],
   "source": [
    "#https://huggingface.co/studio-ousia/mluke-base\n",
    "evaluate_lm(examples, 'studio-ousia/mluke-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at sdadas/polish-longformer-base-4096 were not used when initializing LongformerForMaskedLM: ['longformer.embeddings.position_ids']\n",
      "- This IS expected if you are initializing LongformerForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PRZYPADKI: \"Ta >>><<< jest bardzo mocno napompowana.\" <- ['woda', 'ziemia', 'rzeka', 'sala', 'butelka']\n",
      "PRZYPADKI: \"Naucz się >>><<< na jutrzejszy sprawdzian.\" <- ['pisać', 'chodzić', 'odpowiadać', 'przygotowywać', 'angielskiego']\n",
      "PRZYPADKI: \"Prawdziwemu >>><<< zawsze można ufać.\" <- ['człowiekowi', 'Bogu', 'nie', 'mężowi', 'królowi']\n",
      "PRZYPADKI: \"W deszczowy dzień zabierz ze sobą >>><<<.\" <- ['parasol', 'wodę', 'płaszcz', 'deszcz', 'kapelusz']\n",
      "PRZYPADKI: \"Uwielbiam spędzać czas z >>><<<.\" <- ['tobą', 'dziećmi', 'nimi', 'ludźmi', 'Tobą']\n",
      "PRZYPADKI: \"Opowiadanie o >>><<< zawsze doprowadza mnie do łez.\" <- ['tym', 'nim', 'nich', 'sobie', 'miłości']\n",
      "PRZYPADKI: \"Szanowna >>><<<, nawiązując do otrzymanych zastrzeżeń...\" <- ['Pani', 'pani', 'Premier', 'Rado', 'Minister']\n",
      "\n",
      "RELACJA: \"Gdy odbierasz paczkę, sprawdź >>><<< jeszcze przy kurierze.\" <- ['coś', 'raz', 'to', 'ją', 'j']\n",
      "RELACJA: \"Mijając ludzi na ulicy nie przyglądaj się >>><<< zbyt natarczywie.\" <- ['im', 'jej', 'nikomu', 'temu', 'mu']\n",
      "RELACJA: \"Gdy już go znajdziesz, zrób z >>><<< co uważasz za stosowne.\" <- ['nim', 'tym', 'nią', 'nimi', 'sobą']\n",
      "\n",
      "WIEDZA: \"Stolicą Niemiec jest >>><<<.\" <- ['Berlin', 'Hamburg', 'Bonn', 'Frankfurt', 'Monachium']\n",
      "WIEDZA: \"Ziemia jest >>><<< planetą od Słońca.\" <- ['mniejszą', 'większą', 'niższą', 'inną', 'wyższą']\n",
      "WIEDZA: \">>><<< jest procesem zmiany ze stanu ciekłego w stan gazowy.\" <- ['Gaz', 'Proces', 'Cykl', 'Woda', 'Zmiana']\n",
      "WIEDZA: \"Wisła uchodzi do >>><<<.\" <- ['Bałtyku', 'morza', 'Odry', 'Wisły', 'Atlantyku']\n",
      "\n",
      "NACECHOWANIE: \"'Jestem w niej zakochany na zabój'  jest zdaniem wyrażającym >>><<<.\" <- ['miłość', 'uczucie', 'uczucia', 'przekonanie', 'niepokój']\n",
      "NACECHOWANIE: \"'Motyla noga, nigdy nie znajdę już tego świerszcza' powiedziała osoba o nastawieniu >>><<<.\" <- ['ekologicznym', 'religijnym', 'LGBT', 'negatywnym', 'przeciwnym']\n",
      "NACECHOWANIE: \"'Jak ja nie cierpię tych okropnych Smerfów' to zdanie przedstawiające >>><<< emocje.\" <- ['moje', 'ludzkie', 'jego', 'nasze', 'ich']\n",
      "NACECHOWANIE: \"'Hura, nareszcie zdałem do kolejnej klasy' powiedziała osoba czująca się >>><<<.\" <- ['dobrze', 'lepiej', 'świetnie', 'najlepiej', 'znakomicie']\n",
      "NACECHOWANIE: \"'Niech on tylko wpadnie w moje ręce' spośród emocji pozytywna, neutralna i negatywna, najbliżej temu zdaniu jest emocja >>><<<.\" <- ['strachu', 'negatywna', 'pozytywna', 'miłości', 'dobra']\n"
     ]
    }
   ],
   "source": [
    "#https://huggingface.co/sdadas/polish-longformer-base-4096\n",
    "evaluate_lm(examples, 'sdadas/polish-longformer-base-4096')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PRZYPADKI: \"Ta >>><<< jest bardzo mocno napompowana.\" <- ['łódź', 'Titan', '164', 'rzeka', 'cover']\n",
      "PRZYPADKI: \"Naucz się >>><<< na jutrzejszy sprawdzian.\" <- ['cierpliwość', 'taktyki', 'poprawnie', 'metody', 'pisanie']\n",
      "PRZYPADKI: \"Prawdziwemu >>><<< zawsze można ufać.\" <- ['uczciwość', 'zaufanie', 'prawdy', 'prawdziwe', '183']\n",
      "PRZYPADKI: \"W deszczowy dzień zabierz ze sobą >>><<<.\" <- ['ubrania', 'plecak', 'chleb', 'prezenty', 'rower']\n",
      "PRZYPADKI: \"Uwielbiam spędzać czas z >>><<<.\" <- ['dziewczyny', 'kobiety', 'Paris', 'Sophie', 'ciebie']\n",
      "PRZYPADKI: \"Opowiadanie o >>><<< zawsze doprowadza mnie do łez.\" <- ['gwałt', 'Spider', 'autor', 'Potter', 'history']\n",
      "PRZYPADKI: \"Szanowna >>><<<, nawiązując do otrzymanych zastrzeżeń...\" <- ['uprzejmie', 'pani', '183', 'prośby', 'Sophie']\n",
      "\n",
      "RELACJA: \"Gdy odbierasz paczkę, sprawdź >>><<< jeszcze przy kurierze.\" <- ['paczki', 'zawartość', 'odbiór', 'etykiet', 'sprawdzenie']\n",
      "RELACJA: \"Mijając ludzi na ulicy nie przyglądaj się >>><<< zbyt natarczywie.\" <- ['krzywdy', 'jazda', 'Action', 'strażnik', 'widok']\n",
      "RELACJA: \"Gdy już go znajdziesz, zrób z >>><<< co uważasz za stosowne.\" <- ['nim', 'opis', 'opisem', 'reszta', 'próbki']\n",
      "\n",
      "WIEDZA: \"Stolicą Niemiec jest >>><<<.\" <- ['Berlin', 'Frankfurt', 'Monachium', 'Amsterdam', 'Wiedeń']\n",
      "WIEDZA: \"Ziemia jest >>><<< planetą od Słońca.\" <- ['oddalenie', 'odmienne', 'oddzielone', 'oddzielnie', 'biegun']\n",
      "WIEDZA: \">>><<< jest procesem zmiany ze stanu ciekłego w stan gazowy.\" <- ['przemian', 'czyszczenie', 'proces', '.', 'reakcja']\n",
      "WIEDZA: \"Wisła uchodzi do >>><<<.\" <- ['rzeka', 'Urugwaju', 'River', 'rzeki', 'Wisły']\n",
      "\n",
      "NACECHOWANIE: \"'Jestem w niej zakochany na zabój'  jest zdaniem wyrażającym >>><<<.\" <- ['miłość', 'love', 'zazdrość', 'Amor', 'wierność']\n",
      "NACECHOWANIE: \"'Motyla noga, nigdy nie znajdę już tego świerszcza' powiedziała osoba o nastawieniu >>><<<.\" <- ['łowieckie', 'muzyka', 'nastawienie', 'egetarian', 'cover']\n",
      "NACECHOWANIE: \"'Jak ja nie cierpię tych okropnych Smerfów' to zdanie przedstawiające >>><<< emocje.\" <- ['pozytywne', 'spokojne', 'emo', 'identyczne', 'skrajne']\n",
      "NACECHOWANIE: \"'Hura, nareszcie zdałem do kolejnej klasy' powiedziała osoba czująca się >>><<<.\" <- ['szczęśliwe', 'porażki', 'wstyd', 'sukces', 'autor']\n",
      "NACECHOWANIE: \"'Niech on tylko wpadnie w moje ręce' spośród emocji pozytywna, neutralna i negatywna, najbliżej temu zdaniu jest emocja >>><<<.\" <- ['emo', '0.', 'synonim', 'pozytywne', 'neutralne']\n"
     ]
    }
   ],
   "source": [
    "#https://huggingface.co/sdadas/polish-splade\n",
    "evaluate_lm(examples, 'sdadas/polish-splade')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PRZYPADKI: \"Ta >>><<< jest bardzo mocno napompowana.\" <- ['woda', 'kawa', 'zupa', 'skóra', 'ryba']\n",
      "PRZYPADKI: \"Naucz się >>><<< na jutrzejszy sprawdzian.\" <- ['liczyć', 'przygotować', 'pisać', 'odpowiadać', 'matematyki']\n",
      "PRZYPADKI: \"Prawdziwemu >>><<< zawsze można ufać.\" <- ['Bogu', 'nie', 'człowiekowi', 'dziecku', 'Panu']\n",
      "PRZYPADKI: \"W deszczowy dzień zabierz ze sobą >>><<<.\" <- ['słońce', 'książkę', 'śnieg', 'wodę', 'kwiaty']\n",
      "PRZYPADKI: \"Uwielbiam spędzać czas z >>><<<.\" <- ['przyjaciółmi', 'rodziną', 'dziećmi', 'dzieckiem', 'ludźmi']\n",
      "PRZYPADKI: \"Opowiadanie o >>><<< zawsze doprowadza mnie do łez.\" <- ['miłości', 'tobie', 'sobie', 'przeszłości', 'ludziach']\n",
      "PRZYPADKI: \"Szanowna >>><<<, nawiązując do otrzymanych zastrzeżeń...\" <- ['Pani', 'pani', 'Państwo', 'Panie', 'Panowie']\n",
      "\n",
      "RELACJA: \"Gdy odbierasz paczkę, sprawdź >>><<< jeszcze przy kurierze.\" <- ['ją', 'czy', 'się', 'je', 'to']\n",
      "RELACJA: \"Mijając ludzi na ulicy nie przyglądaj się >>><<< zbyt natarczywie.\" <- ['im', 'nim', 'tym', 'nam', 'ich']\n",
      "RELACJA: \"Gdy już go znajdziesz, zrób z >>><<< co uważasz za stosowne.\" <- ['niego', 'nim', 'tego', 'tym', 'siebie']\n",
      "\n",
      "WIEDZA: \"Stolicą Niemiec jest >>><<<.\" <- ['Berlin', 'Warszawa', 'Frankfurt', 'Bonn', 'Weimar']\n",
      "WIEDZA: \"Ziemia jest >>><<< planetą od Słońca.\" <- ['mniejszą', 'inną', 'drugą', 'większą', 'jedną']\n",
      "WIEDZA: \">>><<< jest procesem zmiany ze stanu ciekłego w stan gazowy.\" <- ['Temperatura', 'Proces', '</s>', 'Woda', 'LPG']\n",
      "WIEDZA: \"Wisła uchodzi do >>><<<.\" <- ['gry', 'finału', 'zwycięstwa', 'Warszawy', 'Krakowa']\n",
      "\n",
      "NACECHOWANIE: \"'Jestem w niej zakochany na zabój'  jest zdaniem wyrażającym >>><<<.\" <- ['miłość', 'uczucia', 'się', 'zainteresowanie', 'to']\n",
      "NACECHOWANIE: \"'Motyla noga, nigdy nie znajdę już tego świerszcza' powiedziała osoba o nastawieniu >>><<<.\" <- ['.', 'mózgu', 'alkoholu', 'mocy', 'umysłu']\n",
      "NACECHOWANIE: \"'Jak ja nie cierpię tych okropnych Smerfów' to zdanie przedstawiające >>><<< emocje.\" <- ['moje', 'ich', 'ludzkie', 'prawdziwe', 'te']\n",
      "NACECHOWANIE: \"'Hura, nareszcie zdałem do kolejnej klasy' powiedziała osoba czująca się >>><<<.\" <- ['dziwnie', 'świetnie', 'dobrze', 'bezpiecznie', 'doskonale']\n",
      "NACECHOWANIE: \"'Niech on tylko wpadnie w moje ręce' spośród emocji pozytywna, neutralna i negatywna, najbliżej temu zdaniu jest emocja >>><<<.\" <- ['zewnętrzna', 'wewnętrzna', 'społeczna', 'dodatkowa', 'zła']\n"
     ]
    }
   ],
   "source": [
    "#https://huggingface.co/facebook/xlm-v-base\n",
    "evaluate_lm(examples, 'facebook/xlm-v-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PRZYPADKI: \"Ta >>><<< jest bardzo mocno napompowana.\" <- ['strona', 'maszyna', 'dziewczyna', 'scena', 'kobieta']\n",
      "PRZYPADKI: \"Naucz się >>><<< na jutrzejszy sprawdzian.\" <- ['chodzić', 'jeździć', 'materiału', 'iść', 'języka']\n",
      "PRZYPADKI: \"Prawdziwemu >>><<< zawsze można ufać.\" <- ['człowiekowi', 'bogu', 'panu', 'światu', 'życiu']\n",
      "PRZYPADKI: \"W deszczowy dzień zabierz ze sobą >>><<<.\" <- ['broń', 'plecak', 'rower', 'jedzenie', 'samochód']\n",
      "PRZYPADKI: \"Uwielbiam spędzać czas z >>><<<.\" <- ['rodziną', 'przyjaciółmi', 'tobą', 'dziećmi', 'ludźmi']\n",
      "PRZYPADKI: \"Opowiadanie o >>><<< zawsze doprowadza mnie do łez.\" <- ['tym', 'mnie', 'nim', 'sobie', 'tobie']\n",
      "PRZYPADKI: \"Szanowna >>><<<, nawiązując do otrzymanych zastrzeżeń...\" <- ['pani', 'państwo', 'panie', 'spółka', 'redakcja']\n",
      "\n",
      "RELACJA: \"Gdy odbierasz paczkę, sprawdź >>><<< jeszcze przy kurierze.\" <- ['ją', 'to', 'go', 'coś', 'je']\n",
      "RELACJA: \"Mijając ludzi na ulicy nie przyglądaj się >>><<< zbyt natarczywie.\" <- ['im', 'temu', 'ludziom', 'tym', 'nam']\n",
      "RELACJA: \"Gdy już go znajdziesz, zrób z >>><<< co uważasz za stosowne.\" <- ['nim', 'tym', 'niego', 'nami', 'tego']\n",
      "\n",
      "WIEDZA: \"Stolicą Niemiec jest >>><<<.\" <- ['londyn', 'berlin', 'paryż', 'warszawa', 'polska']\n",
      "WIEDZA: \"Ziemia jest >>><<< planetą od Słońca.\" <- ['lepszą', 'większą', 'drugą', 'wolną', 'trzecią']\n",
      "WIEDZA: \">>><<< jest procesem zmiany ze stanu ciekłego w stan gazowy.\" <- ['gaz', 'proces', 'zmiana', 'praca', 'co']\n",
      "WIEDZA: \"Wisła uchodzi do >>><<<.\" <- ['wisły', 'góry', 'morza', 'rzeki', 'nieba']\n",
      "\n",
      "NACECHOWANIE: \"'Jestem w niej zakochany na zabój'  jest zdaniem wyrażającym >>><<<.\" <- ['miłość', 'prawdę', 'nadzieję', 'żal', 'smutek']\n",
      "NACECHOWANIE: \"'Motyla noga, nigdy nie znajdę już tego świerszcza' powiedziała osoba o nastawieniu >>><<<.\" <- ['serca', 'zwierząt', 'człowieka', 'psa', 'ciała']\n",
      "NACECHOWANIE: \"'Jak ja nie cierpię tych okropnych Smerfów' to zdanie przedstawiające >>><<< emocje.\" <- ['moje', 'ludzkie', 'nasze', 'prawdziwe', 'twoje']\n",
      "NACECHOWANIE: \"'Hura, nareszcie zdałem do kolejnej klasy' powiedziała osoba czująca się >>><<<.\" <- ['lepiej', 'bezpiecznie', 'źle', 'dobrze', 'bezpieczna']\n",
      "NACECHOWANIE: \"'Niech on tylko wpadnie w moje ręce' spośród emocji pozytywna, neutralna i negatywna, najbliżej temu zdaniu jest emocja >>><<<.\" <- ['prawdziwa', 'zła', 'miłości', 'dobra', 'własna']\n"
     ]
    }
   ],
   "source": [
    "#https://huggingface.co/OrlikB/KartonBERT_base_uncased_v1\n",
    "evaluate_lm(examples, 'OrlikB/KartonBERT_base_uncased_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PRZYPADKI: \"Ta >>><<< jest bardzo mocno napompowana.\" <- [' nie', ' sytuacja', ' sprawa', ' społka', 'cja']\n",
      "PRZYPADKI: \"Naucz się >>><<< na jutrzejszy sprawdzian.\" <- [' wiecej', ' tez', ' takze', ' rowniez', ' tylko']\n",
      "PRZYPADKI: \"Prawdziwemu >>><<< zawsze można ufać.\" <- [' sobie', ' panstwu', ' nie', ' obrazu', ',']\n",
      "PRZYPADKI: \"W deszczowy dzień zabierz ze sobą >>><<<.\" <- [' ”', ' \"', '!', '?', ' »']\n",
      "PRZYPADKI: \"Uwielbiam spędzać czas z >>><<<.\" <- [' niemiec', ' polska', ' belgii', ' luksemburga', ' polski']\n",
      "PRZYPADKI: \"Opowiadanie o >>><<< zawsze doprowadza mnie do łez.\" <- [' tym', ' nim', ' nie', ' polsce', ' kosztach']\n",
      "PRZYPADKI: \"Szanowna >>><<<, nawiązując do otrzymanych zastrzeżeń...\" <- [' pani', ' pan', ' minister', ' ministra', ' pana']\n",
      "\n",
      "RELACJA: \"Gdy odbierasz paczkę, sprawdź >>><<< jeszcze przy kurierze.\" <- ['anie', ' ja', ' sie', ' je', 'cie']\n",
      "RELACJA: \"Mijając ludzi na ulicy nie przyglądaj się >>><<< zbyt natarczywie.\" <- [' juz', ' tu', ' na', ' do', ' mi']\n",
      "RELACJA: \"Gdy już go znajdziesz, zrób z >>><<< co uważasz za stosowne.\" <- [' nim', ' niego', ' tego', ' urzedu', ' tym']\n",
      "\n",
      "WIEDZA: \"Stolicą Niemiec jest >>><<<.\" <- [' niedopuszczalna', ' oddalona', ' dopuszczalna', ' obciazona', ' ostateczna']\n",
      "WIEDZA: \"Ziemia jest >>><<< planetą od Słońca.\" <- [' zatem', ' głownie', ' inna', ' najbardziej', ' ograniczona']\n",
      "WIEDZA: \">>><<< jest procesem zmiany ze stanu ciekłego w stan gazowy.\" <- [' sie', ' produkt', ' elektrycznej', ' ktory', ' przepływ']\n",
      "WIEDZA: \"Wisła uchodzi do >>><<<.\" <- [' niemiec', ' polski', ' turcji', ' polska', ' irlandii']\n",
      "\n",
      "NACECHOWANIE: \"'Jestem w niej zakochany na zabój'  jest zdaniem wyrażającym >>><<<.\" <- [' zgody', ' opinie', \" '\", ' ”', ' bład']\n",
      "NACECHOWANIE: \"'Motyla noga, nigdy nie znajdę już tego świerszcza' powiedziała osoba o nastawieniu >>><<<.\" <- [' postepowania', ' sprawy', ' pytania', ' obrad', ' sie']\n",
      "NACECHOWANIE: \"'Jak ja nie cierpię tych okropnych Smerfów' to zdanie przedstawiające >>><<< emocje.\" <- [' moja', ' te', ' mi', ' jej', ' rozne']\n",
      "NACECHOWANIE: \"'Hura, nareszcie zdałem do kolejnej klasy' powiedziała osoba czująca się >>><<<.\" <- [' sprawa', ' praca', ' nia', ' szkoła', ' nim']\n",
      "NACECHOWANIE: \"'Niech on tylko wpadnie w moje ręce' spośród emocji pozytywna, neutralna i negatywna, najbliżej temu zdaniu jest emocja >>><<<.\" <- [\" '\", ' prosta', ' kontrolna', ' dobra', ' \"']\n"
     ]
    }
   ],
   "source": [
    "#https://huggingface.co/joelniklaus/legal-xlm-longformer-base\n",
    "evaluate_lm(examples, 'joelniklaus/legal-xlm-longformer-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PRZYPADKI: \"Ta >>><<< jest bardzo mocno napompowana.\" <- ['ta', \"'\", 'nie', '’', '”']\n",
      "PRZYPADKI: \"Naucz się >>><<< na jutrzejszy sprawdzian.\" <- ['##c', '##ca', '##wa', '##pic', '##cza']\n",
      "PRZYPADKI: \"Prawdziwemu >>><<< zawsze można ufać.\" <- ['.', 'nie', 'mozna', 'sa', ',']\n",
      "PRZYPADKI: \"W deszczowy dzień zabierz ze sobą >>><<<.\" <- ['m', 'h', 'd', 'a', 's']\n",
      "PRZYPADKI: \"Uwielbiam spędzać czas z >>><<<.\" <- ['1', '2', 'r', '3', '8']\n",
      "PRZYPADKI: \"Opowiadanie o >>><<< zawsze doprowadza mnie do łez.\" <- ['wartosci', 'tej', 'charakterze', 'niej', 'tym']\n",
      "PRZYPADKI: \"Szanowna >>><<<, nawiązując do otrzymanych zastrzeżeń...\" <- ['republika', 'panstwa', 'strone', 'pytanie', 'region']\n",
      "\n",
      "RELACJA: \"Gdy odbierasz paczkę, sprawdź >>><<< jeszcze przy kurierze.\" <- ['##ajac', '##ac', '##ie', '##a', '##i']\n",
      "RELACJA: \"Mijając ludzi na ulicy nie przyglądaj się >>><<< zbyt natarczywie.\" <- ['sie', '.', 'na', ',', 'rowniez']\n",
      "RELACJA: \"Gdy już go znajdziesz, zrób z >>><<< co uważasz za stosowne.\" <- ['tego', 'tym', 'nimi', 'nich', 'roku']\n",
      "\n",
      "WIEDZA: \"Stolicą Niemiec jest >>><<<.\" <- ['1', '0', '2', '3', '4']\n",
      "WIEDZA: \"Ziemia jest >>><<< planetą od Słońca.\" <- ['strona', 'czesci', 'czesc', 'punkt', 'obecnie']\n",
      "WIEDZA: \">>><<< jest procesem zmiany ze stanu ciekłego w stan gazowy.\" <- ['##owany', '##any', '.', '##owane', 'i']\n",
      "WIEDZA: \"Wisła uchodzi do >>><<<.\" <- ['ue', 'art', 'europy', '2001', 'zob']\n",
      "\n",
      "NACECHOWANIE: \"'Jestem w niej zakochany na zabój'  jest zdaniem wyrażającym >>><<<.\" <- ['6', '5', 'ds', '4', 'art']\n",
      "NACECHOWANIE: \"'Motyla noga, nigdy nie znajdę już tego świerszcza' powiedziała osoba o nastawieniu >>><<<.\" <- ['1', '15', '14', '2', '5']\n",
      "NACECHOWANIE: \"'Jak ja nie cierpię tych okropnych Smerfów' to zdanie przedstawiające >>><<< emocje.\" <- ['sie', 'nie', 'jest', 'wzor', 'sa']\n",
      "NACECHOWANIE: \"'Hura, nareszcie zdałem do kolejnej klasy' powiedziała osoba czująca się >>><<<.\" <- ['m', '1', \"'\", '\"', 'sci']\n",
      "NACECHOWANIE: \"'Niech on tylko wpadnie w moje ręce' spośród emocji pozytywna, neutralna i negatywna, najbliżej temu zdaniu jest emocja >>><<<.\" <- ['1', '6', '2', '3', '5']\n"
     ]
    }
   ],
   "source": [
    "#https://huggingface.co/EuropeanParliament/EUBERT\n",
    "evaluate_lm(examples, 'EuropeanParliament/EUBERT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PRZYPADKI: \"Ta >>><<< jest bardzo mocno napompowana.\" <- ['strona', 'maszyna', 'dziewczyna', 'scena', 'kobieta']\n",
      "PRZYPADKI: \"Naucz się >>><<< na jutrzejszy sprawdzian.\" <- ['chodzić', 'jeździć', 'materiału', 'iść', 'języka']\n",
      "PRZYPADKI: \"Prawdziwemu >>><<< zawsze można ufać.\" <- ['człowiekowi', 'bogu', 'panu', 'światu', 'życiu']\n",
      "PRZYPADKI: \"W deszczowy dzień zabierz ze sobą >>><<<.\" <- ['broń', 'plecak', 'rower', 'jedzenie', 'samochód']\n",
      "PRZYPADKI: \"Uwielbiam spędzać czas z >>><<<.\" <- ['rodziną', 'przyjaciółmi', 'tobą', 'dziećmi', 'ludźmi']\n",
      "PRZYPADKI: \"Opowiadanie o >>><<< zawsze doprowadza mnie do łez.\" <- ['tym', 'mnie', 'nim', 'sobie', 'tobie']\n",
      "PRZYPADKI: \"Szanowna >>><<<, nawiązując do otrzymanych zastrzeżeń...\" <- ['pani', 'państwo', 'panie', 'spółka', 'redakcja']\n",
      "\n",
      "RELACJA: \"Gdy odbierasz paczkę, sprawdź >>><<< jeszcze przy kurierze.\" <- ['ją', 'to', 'go', 'coś', 'je']\n",
      "RELACJA: \"Mijając ludzi na ulicy nie przyglądaj się >>><<< zbyt natarczywie.\" <- ['im', 'temu', 'ludziom', 'tym', 'nam']\n",
      "RELACJA: \"Gdy już go znajdziesz, zrób z >>><<< co uważasz za stosowne.\" <- ['nim', 'tym', 'niego', 'nami', 'tego']\n",
      "\n",
      "WIEDZA: \"Stolicą Niemiec jest >>><<<.\" <- ['londyn', 'berlin', 'paryż', 'warszawa', 'polska']\n",
      "WIEDZA: \"Ziemia jest >>><<< planetą od Słońca.\" <- ['lepszą', 'większą', 'drugą', 'wolną', 'trzecią']\n",
      "WIEDZA: \">>><<< jest procesem zmiany ze stanu ciekłego w stan gazowy.\" <- ['gaz', 'proces', 'zmiana', 'praca', 'co']\n",
      "WIEDZA: \"Wisła uchodzi do >>><<<.\" <- ['wisły', 'góry', 'morza', 'rzeki', 'nieba']\n",
      "\n",
      "NACECHOWANIE: \"'Jestem w niej zakochany na zabój'  jest zdaniem wyrażającym >>><<<.\" <- ['miłość', 'prawdę', 'nadzieję', 'żal', 'smutek']\n",
      "NACECHOWANIE: \"'Motyla noga, nigdy nie znajdę już tego świerszcza' powiedziała osoba o nastawieniu >>><<<.\" <- ['serca', 'zwierząt', 'człowieka', 'psa', 'ciała']\n",
      "NACECHOWANIE: \"'Jak ja nie cierpię tych okropnych Smerfów' to zdanie przedstawiające >>><<< emocje.\" <- ['moje', 'ludzkie', 'nasze', 'prawdziwe', 'twoje']\n",
      "NACECHOWANIE: \"'Hura, nareszcie zdałem do kolejnej klasy' powiedziała osoba czująca się >>><<<.\" <- ['lepiej', 'bezpiecznie', 'źle', 'dobrze', 'bezpieczna']\n",
      "NACECHOWANIE: \"'Niech on tylko wpadnie w moje ręce' spośród emocji pozytywna, neutralna i negatywna, najbliżej temu zdaniu jest emocja >>><<<.\" <- ['prawdziwa', 'zła', 'miłości', 'dobra', 'własna']\n"
     ]
    }
   ],
   "source": [
    "#https://huggingface.co/OrlikB/KartonBERT_base_uncased_v1\n",
    "evaluate_lm(examples, 'OrlikB/KartonBERT_base_uncased_v1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
