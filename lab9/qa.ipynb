{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marcin Wardyński  \n",
    "czwartek, 8:00\n",
    "\n",
    "## Lab 9: Kontekstowe odpowiadanie na pytania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do przygotowania danych treningowych i walidacyjnych dla modelu trzeba przerobić istniejące pliki json. Format nowych plików musi zawierać elementy:\n",
    "- `id`\n",
    "- `context`\n",
    "- `question`\n",
    "- `answers` z polem `text` zawierającym w postaci listy poprawne odpowiedzi\n",
    "\n",
    "Poniższa funkcja dokonuje oczekiwanego przekształcenia na zbiorze PoQuAD: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def convert_data(data):\n",
    "    results = []\n",
    "    i = 0\n",
    "    for article in data.get(\"data\", []):\n",
    "        for paragraph in article.get(\"paragraphs\", []):\n",
    "            context = paragraph[\"context\"]\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                question = qa[\"question\"]\n",
    "                answers = qa['answers'] if 'answers' in qa.keys() else qa['plausible_answers']\n",
    "                for answer in answers:\n",
    "                    i += 1\n",
    "                    results.append({\n",
    "                        \"id\": i,\n",
    "                        \"context\": context,\n",
    "                        \"question\": question,\n",
    "                        \"answers\": {\n",
    "                            \"text\": [answer[\"generative_answer\"]]\n",
    "                        }\n",
    "                    })\n",
    "    return results\n",
    "\n",
    "\n",
    "def convert_format(input_filepath, output_filepath):\n",
    "    with open(input_filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    output_data = convert_data(data)\n",
    "    output_wrapped_data = {\"version\": \"0.1.0\", \"data\": output_data}\n",
    "\n",
    "    with open(output_filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(output_wrapped_data, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wywołuję powyższą funkcję i zapisuję przekształcone dane w plikach `poquad-conv-train.json` oraz `poquad-conv-dev.json` dla zbiorów treningowego i walidacyjnego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_format(\"poquad-train.json\", \"poquad-conv-train.json\")\n",
    "convert_format(\"poquad-dev.json\", \"poquad-conv-dev.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do fine-tuningu swojego modelu użyłem nieznacznie zmienionego skryptu `run_seq2seq_qa.py` z repozytorium `transformers`, który został wywołany z następującymi parametrami:\n",
    "```\n",
    "  --model_name_or_path allegro/plt5-base \\\n",
    "  --train_file ../../../..poquad-conv-train.json \\\n",
    "  --validation_file ../../../..poquad-conv-dev.json \\\n",
    "  --context_column context \\\n",
    "  --question_column question \\\n",
    "  --answer_column answers \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --per_device_train_batch_size X \\\n",
    "  --learning_rate Ye-5 \\\n",
    "  --num_train_epochs 3 \\\n",
    "  --eval_strategy steps \\\n",
    "  --eval_steps 500 \\\n",
    "  --save_steps 500 \\\n",
    "  --predict_with_generate True \\\n",
    "  --metric_for_best_model f1 \\\n",
    "  --greater_is_better True \\\n",
    "  --load_best_model_at_end True \\\n",
    "  --save_total_limit 3 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --output_dir ../../../../model_poquad_t5_base_f1_3e-5_b12\n",
    "```\n",
    "\n",
    "Jak widzimy powyżej, jako modelu pretrenowanego używam `allegro/plt5-base`, gdyż moja konfiguracja komputera nie pozwala mi na wykorzystanie pojemniejszego modelu. Do treningu przekazuję uprzednio przygotowane pliki ze zbiorem danych PoQuAD oraz wskazuję nazwy odpowiednich kolumn. Zlecam wykonanie treningu i ewaluacji z wielkością batcha i krokiem treningu zdefiniowanym dla każdego uruchomienia skryptu z innymi wartościami. Trening ma trwać zalecane trzy epoki, a wybrana strategia ewaluacji `f1` powinna zostać uruchomiona co 500 kroków. Finalnie powinien zostać zaladowany model o najlepszym wyniku ewaluacji. Parametry `max_seq_length` oraz `doc_stride` otrzymały wartości odpowiednio 384 i 128."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wyniki skyptu dla wybranych parametrów treningowych:\n",
    "\n",
    "| lr       | #batchy | eval em   | eval f1   |\n",
    "|----------|---------|-----------|-----------|\n",
    "| 2e-5     | 8       | 51.53     | 67.62     |\n",
    "| 3e-5     | 12      | 50.84     | 66.87     |\n",
    "| 4e-5     | 16      | 51.39     | 67.25     |\n",
    "| **5e-5** | **16**  | **52.35** | **68.30** |\n",
    "\n",
    "Jak widzimy, dla danych walidacyjnych zbioru PoQuAD najlepiej wypada model o stałej uczącej `5e-5` i rozmiarze batch-a `16` i to zarówno dla metryki *Exact Match*, jak i *f1*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skoro mamy już wytrenowany model wraz z jego wstępną ewaluacją, skupmy się na danych testowych. Poniższy zestaw funkcji buduje struktury słownikowe, które ułatwiają nawigowanie po danych testowych zawierających wskazane pytania prawne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "NO_ANS = \"no_ans\"\n",
    "\n",
    "class QA:\n",
    "    def __init__(self, question_id, question, answer):\n",
    "        self.question_id = question_id\n",
    "        self.question = question\n",
    "        self.answer = answer\n",
    "\n",
    "class Entry:\n",
    "    def __init__(self, passage_id, passage_text, qas):\n",
    "        self.passage_id = passage_id\n",
    "        self.passage_text = passage_text\n",
    "        self.qas = qas\n",
    "\n",
    "def init_qas_with_answers(filepath):\n",
    "    qa_dict = {}\n",
    "    with open(filepath, \"r\") as file:\n",
    "        for line in file:\n",
    "            record = json.loads(line.strip())\n",
    "            \n",
    "            if \"score\" in record and record[\"score\"] == \"1\"\\\n",
    "                    and \"question-id\" in record and \"answer\" in record:\n",
    "                qa = QA(record[\"question-id\"], None, record[\"answer\"])\n",
    "                qa_dict[record[\"question-id\"]] = qa\n",
    "    return qa_dict\n",
    "\n",
    "def match_questions_to_answers(filepath, qa_dict):\n",
    "    q_wo_a = []\n",
    "    with open(filepath, \"r\") as file:\n",
    "        for line in file:\n",
    "            record = json.loads(line.strip())\n",
    "            \n",
    "            if \"text\" in record and \"_id\" in record:\n",
    "                if record[\"_id\"] in qa_dict.keys():\n",
    "                    qa = qa_dict[record[\"_id\"]]\n",
    "                    qa.question = record[\"text\"]\n",
    "                else:\n",
    "                    qa_dict[NO_ANS].append(record[\"text\"])\n",
    "\n",
    "\n",
    "def organize_question_to_context_relations(filepath, qa_dict):\n",
    "    qc_dict = {}\n",
    "    with open(filepath, \"r\") as file:\n",
    "        for line in file:\n",
    "            record = json.loads(line.strip())\n",
    "            \n",
    "            if \"score\" in record and record[\"score\"] == \"1\"\\\n",
    "                    and \"passage-id\" in record\\\n",
    "                    and \"question-id\" in record and record[\"question-id\"] in qa_dict.keys():\n",
    "                if record[\"passage-id\"] not in qc_dict.keys():\n",
    "                    qc_dict[record[\"passage-id\"]] = []\n",
    "                qc_dict[record[\"passage-id\"]].append(record[\"question-id\"])\n",
    "    return qc_dict\n",
    "\n",
    "def load_passages(filepath, qc_dict, qa_dict):\n",
    "    entries = []\n",
    "    with open(filepath, \"r\") as file:\n",
    "        for line in file:\n",
    "            record = json.loads(line.strip())\n",
    "            \n",
    "            if \"text\" in record and \"_id\" in record and record[\"_id\"] in qc_dict.keys():\n",
    "                qa_ids = qc_dict[record[\"_id\"]]\n",
    "                qas = []\n",
    "                for qa_id in qa_ids:\n",
    "                    qas.append(qa_dict[qa_id])\n",
    "                entries.append(Entry(record[\"_id\"], record[\"text\"], qas))\n",
    "    return entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po uruchomieniu tych fynkcji i przekazaniu odpowiednich plików wejściowych otrzymujemy następujące struktury:  \n",
    "`qa_dict` - słownik łączący pytania z odpowiedziami  \n",
    "`qc_dict` - słownik łączący pytania z kontekstem  \n",
    "`test_passages` - lista z kontekstem oraz połączonymi z nim pytaniami i odpowiedziami  \n",
    "\n",
    "Tak przygotowanych struktur będą wykorzystywać, aby zewaluować jakość modelu dla danych testowych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_dict = init_qas_with_answers(\"simple-legal-questions-pl-main/answers.jl\")\n",
    "\n",
    "qa_dict[NO_ANS] = []\n",
    "match_questions_to_answers(\"simple-legal-questions-pl-main/questions.jl\", qa_dict)\n",
    "\n",
    "qc_dict = organize_question_to_context_relations(\"simple-legal-questions-pl-main/relevant.jl\", qa_dict)\n",
    "test_passages = load_passages(\"simple-legal-questions-pl-main/passages.jl\", qc_dict, qa_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dodatkowo napisałem funkcję przekształcającą dane zbioru PoQuAD do reprezentacji odpowiadającej danym testowym. Napisałem ją, gdyż chciałbym dodatkowo policzyć metryki dla zbioru walidacyjnego dokładnie tymi samymi funkcjami, co dla zbioru testowego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_poquad_data(filepath):\n",
    "    val_entries = []\n",
    "    with open(filepath, \"r\") as file:\n",
    "        json_content = json.load(file)\n",
    "        for data in json_content['data']:\n",
    "            for paragraph in data['paragraphs']:\n",
    "                qa_list = []\n",
    "                for qa in paragraph['qas']:\n",
    "                    answers = qa['answers'] if 'answers' in qa.keys() else qa['plausible_answers']\n",
    "                    for answer in answers:\n",
    "                        qa_obj = QA(None, qa['question'], answer['generative_answer'])\n",
    "                        qa_list.append(qa_obj)\n",
    "                    \n",
    "                entry = Entry(None, paragraph['context'], qa_list)\n",
    "                val_entries.append(entry)\n",
    "    return val_entries\n",
    "\n",
    "val_passages = convert_poquad_data(\"poquad-dev.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcja `exec_passages` przechodzi po kontekstach w podanym zbiorze i przekazuje je do modelu wraz z pytaniami i odpowiedziami na nie. Wyjściowo otrzymujemy listę odpowiedzi modelu oraz oczekiwanych odpowiedzi generatywnych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def exec_passages(model, tokenizer, passages):\n",
    "    answers = []\n",
    "    expected_answers = []\n",
    "    for passage in tqdm(passages):\n",
    "        for qa in passage.qas:\n",
    "            input_text = f\"question: {qa.question} context: {passage.passage_text}\"\n",
    "            inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "            outputs = model.generate(inputs[\"input_ids\"], max_length=100, num_beams=5, early_stopping=True)\n",
    "\n",
    "            answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            answers.append(answer)\n",
    "            expected_answers.append(qa.answer)\n",
    "    return answers, expected_answers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz wystarczy wczytać odpowiedni model i wyznaczyć jego odpowiedzi na pytania ze zbioru poprzez wywołanie powyższej funkcji. W pierwszej kolejności wykonuję te działania dla każdego z wytrenowanych modeli na zbiorze walidacyjnym następującymi wywołaniami:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "lr5e_5_b16_t5_base_model_name = \"./model_poquad_t5_base_f1_5e-5_b16\"\n",
    "lr5e_5_b16_t5_base_model_tokenizer = AutoTokenizer.from_pretrained(lr5e_5_b16_t5_base_model_name)\n",
    "lr5e_5_b16_t5_base_model = AutoModelForSeq2SeqLM.from_pretrained(lr5e_5_b16_t5_base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1453/1453 [2:08:29<00:00,  5.31s/it] \n"
     ]
    }
   ],
   "source": [
    "lr5e_5_b16_t5_base_val_answers, expected_val_answers = exec_passages(lr5e_5_b16_t5_base_model, lr5e_5_b16_t5_base_model_tokenizer, val_passages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przedstawiam przykład tylko dla jednego z modeli, reszta została użyta w analogiczny sposób.\n",
    "\n",
    "Następnie definiuję funkcje do obliczania miar *Exact Match* oraz *f1*.\n",
    "\n",
    "Funkcja obliczająca *Exact Match* zawiera mały etap wstępnego przetworzenia tekstów na wejściu, który to usuwa skrajne spacje, znaki interpunkcyjne i zmniejsza czcionkę wszystkich słów, tak żeby lepiej odkrywać dokładne dopasowania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def clean_text(s):\n",
    "    s = s.strip()\n",
    "    s = s.lower()\n",
    "    s = ''.join(c for c in s if c not in string.punctuation)\n",
    "    return s\n",
    "\n",
    "\n",
    "def calculate_exact_matches(answers, expected_answers, clean_fun):\n",
    "    matches = 0\n",
    "    for s1, s2 in zip(answers, expected_answers):\n",
    "        if clean_fun(s1) == clean_fun(s2):\n",
    "            matches += 1\n",
    "    return matches/len(answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ostatecznie funkcja mierząca wartości TP, FP, FN i wyznaczająca na ich podstawie metryki *precision* i *recall* aby zwrócić opierającą się o nie metrykę *f1*. Poza dwoma odpowiedziami funkcja przyjmuje funkcję tokenizującą, która przeprowadza czyszczenie tekstu (takie samo, jak w przypadku *Exact Match*) i dodatkowo tworzy tokeny 1:1 ze słów w zdaniu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    text = clean_text(text)\n",
    "    tokens = re.split(r\"[^\\w]+\", text)\n",
    "    return tokens\n",
    "\n",
    "def compute_single_f1(tokens1, tokens2):\n",
    "    TP = sum((tokens1 & tokens2).values())\n",
    "    FP = sum((tokens1 - tokens2).values())\n",
    "    FN = sum((tokens2 - tokens1).values())\n",
    "\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return f1\n",
    "\n",
    "def compute_f1(answers, expected_answers, tokenize_fun):\n",
    "\n",
    "    f1_scores = []\n",
    "    for s1, s2 in zip(answers, expected_answers):\n",
    "        s1_t = Counter(tokenize_fun(s1))\n",
    "        s2_t = Counter(tokenize_fun(s2))\n",
    "        f1 = compute_single_f1(s1_t, s2_t)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    return sum(f1_scores)/len(f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Następnie wywołuję obydwie funkcje mierzące miary *Exact Match* i *f1* na zbiorze walidacyjnym.  \n",
    "Powtarzam pomiary tych metryk, chociaż zostały mi już one zwrócone podczas treningu, gdyż chcę wykorzystać te same funkcje, których będą wykorzystywal później dla zbioru testowego. Chcę, żeby porównanie wyników dla zbioru walidacyjnego było bardziej spójne.\n",
    "\n",
    "Jak widzimy powyżej, moje implementacje funkcji obliczających te metryki dodatkowo czyszczą wprowadzony tekst, a miara *f1* jest obliczana w sposób mikro (czyli liczymy *f1* dla każdego przypadku z osobna i uśredniamy już wyliczony zbiór wartości *f1*, żeby uzyskać wartość średnią dla zbioru).  \n",
    "Co do funkcji użytych w skrypcie `run_seq2seq_qa.py`, to nie mam pewności w jaki sposób miary te zostały obliczone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.553399433427762"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_exact_matches(lr5e_5_b16_t5_base_val_answers, expected_val_answers, clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7320584815022007"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_f1(lr5e_5_b16_t5_base_val_answers, expected_val_answers, tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wyniki dla wywołań moich funkcji ewaluacyjnych na zbiorze walidacyjnym i dla wszystkich wytrenowanych modeli zamieszczam poniżej:\n",
    "\n",
    "| lr       | #batchy | eval em   | eval f1   |\n",
    "|----------|---------|-----------|-----------|\n",
    "| 2e-5     | 8       | 0.544     | 0.723     |\n",
    "| 3e-5     | 12      | 0.533     | 0.713     |\n",
    "| 4e-5     | 16      | 0.538     | 0.718     |\n",
    "| **5e-5** | **16**  | **0.553** | **0.732** |\n",
    "\n",
    "Widzimy, że faktycznie wyliczone wartości różnią się od tych zwróconych przez skypt treningowy i są odrobinę wyższe. Ma to najpewniej swoje źródło w dodatkowym, minimalnym przygotowaniu danych do porównania, które stosuję.\n",
    "\n",
    "Wciąż najlepszym modelem jest ten o rozmiarze batch-a 16 i stałej uczącej 5e-5.\n",
    "\n",
    "Wiedząc, który model daje najlepsze wyniki dla zbioru walidacyjnego, odpowiedzmy przy jego użyciu na pytania ze zbioru testowego i sprawdźmy rezultaty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 557/557 [14:00<00:00,  1.51s/it]\n"
     ]
    }
   ],
   "source": [
    "lr5e_5_b16_t5_base_answers, expected_answers = exec_passages(lr5e_5_b16_t5_base_model, lr5e_5_b16_t5_base_model_tokenizer, test_passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29442508710801396"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_exact_matches(lr5e_5_b16_t5_base_answers, expected_answers, clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5210907914379148"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_f1(lr5e_5_b16_t5_base_answers, expected_answers, tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otrzymaliśmy następujące wyniki:\n",
    "\n",
    "| lr   | #batchy | eval em | eval f1 |\n",
    "|------|---------|---------|---------|\n",
    "| 5e-5 | 16      | 0.294   | 0.521   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wartości metryk okazały się znacznie słabsze, niż dla zbioru walidacyjnego. Przyjrzyjmy się dziesięcu przykładom bliżej w poszukiwaniu przyczyny gorszych wyników."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:22<00:00,  2.25s/it]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(7)\n",
    "random_passages = random.sample(test_passages, 10)\n",
    "\n",
    "random_passages_answers, expected_random_passages_answers = exec_passages(lr5e_5_b16_t5_base_model, lr5e_5_b16_t5_base_model_tokenizer, random_passages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ponieważ każdy kontekst zawiera tylko jedno pytanie, wyniki dla wylosowanych dziesięciu z nich możemy połączyć z wygenerowanymi odpowiedziami za pomocą liczby porządkowej w następujący sposób:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kontekst: Art. 20. Rada Ministrów, na wniosek Ministra Skarbu Państwa, określa, w drodze rozporządzenia: 1) zakres, szczegółowe zasady i tryb kontroli, o której mowa w art. 2 pkt 7 i 8, 2) tryb powierzania mienia kierownikom urzędów państwowych, 3) tryb powierzania mienia kierownikom jednostek organizacyjnych podporządkowanych kierownikom urzędów państwowych oraz tryb udzielania im pełnomocnictw do reprezentowania Skarbu Państwa, 4) szczegółowe zasady ewidencjonowania majątku Skarbu Państwa, w tym zbiorczej ewidencji, o której mowa w art. 2 pkt 3, oraz związane z tym obowiązki państwowych jednostek organizacyjnych, którym powierzono to mienie.\n",
      "Pytanie: Na czyj wniosek rada ministrów określa tryb powierzania mienia kierownikom urzędów państwowych?\n",
      "Oczekiwana odpowiedź: na wniosek Ministra Skarbu Państwa\n",
      "Wygenerowana odpowiedź: Ministra Skarbu Państwa\n",
      "\n",
      "kontekst: Art. 64. 1. Kontrolę gospodarki finansowej powiatu sprawuje regionalna izba obrachunkowa. 2. Z zastrzeżeniem przepisów tego rozdziału do trybu uchwalania budżetu oraz gospodarki finansowej powiatu stosuje się przepisy odrębnej ustawy. Rozdział 7 Związki, stowarzyszenia i porozumienia powiatów\n",
      "Pytanie: Jakie przepisy stosuje się do gospodarki finansowej związku powiatów?\n",
      "Oczekiwana odpowiedź: Do gospodarki finansowej związku powiatów stosuje się przepisy ustawy o finansach publicznych oraz przepisy wydane na jej podstawie.\n",
      "Wygenerowana odpowiedź: odrębnej ustawy\n",
      "\n",
      "kontekst: Art. 35. 1. Środki ochrony roślin należy stosować tylko sprzętem sprawnym technicznie, który użyty zgodnie z przeznaczeniem zapewni skuteczne zwalczanie organizmów szkodliwych i nie spowoduje szkodliwego wpływu na zdrowie ludzi i zwierząt oraz środowisko. 2. Zabiegi w zakresie stosowania środków ochrony roślin w produkcji towarowej mogą być wykonywane tylko przez osoby przeszkolone przez jednostki organizacyjne upoważnione przez wojewódzkiego inspektora i posiadające świadectwo ukończenia szkolenia.\n",
      "Pytanie: Czy środki ochrony roślin mogą być nakładane poprzez niesprawny technicznie sprzęt?\n",
      "Oczekiwana odpowiedź: Nie\n",
      "Wygenerowana odpowiedź: nie\n",
      "\n",
      "kontekst: Art. 31. 1. Kierowca, który w określonych dniach nie prowadził pojazdu albo prowadził pojazd, do którego nie mają zastosowania przepisy rozporządzenia nr 3820/85/EWG, na żądanie osoby uprawnionej do przeprowadzenia kontroli przedstawia zaświadczenie, które powinno zawierać w szczególności następujące dane: imię i nazwisko kierowcy, okres, którego dotyczy, wskazanie przyczyny nieposiadania wykresówek, o których mowa w art. 15 ust. 7 rozporządzenia nr 3821/85/EWG, miejsce i data wystawienia, podpis pracodawcy. 2. Zaświadczenie, o którym mowa w ust. 1, pracodawca wystawia i wręcza kierowcy przed rozpoczęciem przez kierowcę przewozu drogowego. W przypadku gdy kierowca w określonych dniach nie prowadził pojazdu w trakcie wykonywania danego zadania przewozowego, pracodawca niezwłocznie wystawia i przekazuje zaświadczenie na żądanie osoby uprawnionej do przeprowadzenia kontroli. 3. Przepisy ust. 1 i 2 stosuje się odpowiednio do przedsiębiorcy osobiście wykonującego przewozy drogowe, z tym że przedsiębiorca przedkłada stosowne oświadczenie. Rozdział 5 Przepisy końcowe\n",
      "Pytanie: Które dane powinno zawierać zaświadczenie kierowcy, które on musi przedstawić do przeprowadzenia kontroli?\n",
      "Oczekiwana odpowiedź: Imię i nazwisko kierowcy, okres, którego dotyczy przedstawiane zaświadczenie, wskazanie przyczyny nieposiadania wykresówek, o których mowa w art. 15 ust. 7 rozporządzenia nr 3821/85/EWG, miejsce i data wystawienia przedstawianego zaświadczenia, podpis pracodawcy.\n",
      "Wygenerowana odpowiedź: imię i nazwisko kierowcy, okres, którego dotyczy, wskazanie przyczyny nieposiadania wykresówek, o których mowa w art. 15 ust. 7 rozporządzenia nr 3821/85/EWG\n",
      "\n",
      "kontekst: Art. 36. Prowadzący badania statystyczne statystyki publicznej obowiązani są ogłaszać, udostępniać i rozpowszechniać wynikowe informacje statystyczne z zachowaniem przepisów art. 10 i 14.\n",
      "Pytanie:  Do czego są zobowiązani prowadzący badania statystyczne statystyki publicznej?\n",
      "Oczekiwana odpowiedź: prowadzący badania statystyczne statystyki publicznej są zobowiązani do: przestrzegania zasad etyki zawodowej i ochrony danych osobowych, zapewnienia poufności i bezpieczeństwa przetwarzanych danych, zapewnienia rzetelności i wiarygodności przeprowadzanych badań, udostępnienia informacji o przebiegu i wynikach badań\n",
      "Wygenerowana odpowiedź: ogłaszać, udostępniać i rozpowszechniać wynikowe informacje statystyczne z zachowaniem przepisów art. 10 i 14\n",
      "\n",
      "kontekst: Art. 4. 1. Obszar i granice stref ochronnych powinny być określane w sposób zapewniający Pomnikom Zagłady niezbędną ochronę w sposób jak najmniej uciążliwy dla osób trzecich. Oznakowanie granic stref ochronnych powinno jednoznacznie wskazywać na objęcie oznaczonego pasa gruntu ochroną. 2. Minister właściwy do spraw administracji publicznej określi, w drodze rozporządzenia, sposób oznakowania granic Pomników Zagłady i ich stref ochronnych, a także wzory znaków wskazujących te granice. 3. Minister właściwy do spraw administracji publicznej określi, po zasięgnięciu opinii zarządu właściwej terytorialnie gminy, w drodze rozporządzenia: 1) granice Pomnika Zagłady, na obszarze którego jest położony Pomnik Męczeństwa w Oświęcimiu, zgodnie z jego granicami i obszarem wyznaczonymi na podstawie przepisów ustawy z dnia 2 lipca 1947 r. o upamiętnieniu męczeństwa Narodu Polskiego i innych Narodów w Oświęcimiu (Dz.U. Nr 52, poz. 265), oraz obszar i granice jego strefy ochronnej, 2) granice Pomnika Zagłady, na obszarze którego jest położony Pomnik Męczeństwa na Majdanku, zgodnie z jego granicami i obszarem wyznaczonymi na podstawie przepisów ustawy z dnia 2 lipca 1947 r. o upamiętnieniu męczeństwa Narodu Polskiego i innych Narodów na Majdanku (Dz.U. Nr 52, poz. 266), oraz obszar i granice jego strefy ochronnej, 3) granice Pomnika Zagłady, na obszarze którego jest położone Muzeum \"Stutthof\" w Sztutowie, oraz obszar i granice jego strefy ochronnej, 4) granice Pomnika Zagłady, na obszarze którego jest położone Muzeum GrossRosen w Rogoźnicy, oraz obszar i granice jego strefy ochronnej, 5) granice Pomnika Zagłady, na obszarze którego jest położone Mauzoleum Walki i Męczeństwa w Treblince, oraz obszar i granice jego strefy ochronnej, 6) granice Pomnika Zagłady, na obszarze którego jest położone Muzeum Martyrologiczne - Obóz w Chełmnie nad Nerem, oraz obszar i granice jego strefy ochronnej, 7) granice Pomnika Zagłady, na obszarze którego jest położone Muzeum Byłego Obozu Zagłady w Sobiborze, oraz obszar i granice jego strefy ochronnej, 8) granice Pomnika Zagłady, na obszarze którego jest położony były Obóz Zagłady w Bełżcu, oraz obszar i granice jego strefy ochronnej. 4. Niewyrażenie opinii, o której mowa w ust. 3, w terminie 2 tygodni od dnia zwrócenia się przez ministra właściwego do spraw administracji publicznej o jej wydanie, oznacza wyrażenie opinii pozytywnej. 5. Oznakowanie granic Pomników Zagłady i ich stref ochronnych oraz utrzymanie znaków wskazujących te granice należą do zadań wojewody.\n",
      "Pytanie: Jak duża strefa wokół Pomnika Zagłady jest chroniona?\n",
      "Oczekiwana odpowiedź: Obszar i granice stref ochronnych powinny być określane w sposób zapewniający Pomnikom Zagłady niezbędną ochronę w sposób jak najmniej uciążliwy dla osób trzecich.\n",
      "Wygenerowana odpowiedź: pasa gruntu\n",
      "\n",
      "kontekst: Art. 90. Przy wykonywaniu w aptece czynności fachowych mogą być zatrudnieni wyłącznie farmaceuci i technicy farmaceutyczni w granicach ich uprawnień zawodowych.\n",
      "Pytanie: Kto może być zatrudniony przy wykonywaniu w aptece czynności fachowych?\n",
      "Oczekiwana odpowiedź: Farmaceuci i technicy farmaceutyczni\n",
      "Wygenerowana odpowiedź: farmaceuci i technicy farmaceutyczni\n",
      "\n",
      "kontekst: Art. 111. Komandytariusz odpowiada za zobowiązania spółki wobec jej wierzycieli tylko do wysokości sumy komandytowej.\n",
      "Pytanie: Do jakiej wysokości za zobowiązania spółki odpowiada komandytariusz?\n",
      "Oczekiwana odpowiedź: Komandytariusz odpowiada za zobowiązania spółki wobec jej wierzycieli tylko do wysokości sumy komandytowej.\n",
      "Wygenerowana odpowiedź: sumy komandytowej\n",
      "\n",
      "kontekst: Art. 159. 1. Kadencja Rady upływa wraz z upływem kadencji Prezesa Urzędu. 2. Członkostwo w Radzie wygasa w przypadku upływu kadencji Rady, śmierci członka Rady, jego odwołania albo rezygnacji. 3. Prezes Rady Ministrów odwołuje członka Rady, jeżeli przestał on odpowiadać jednemu z warunków określonych w art. 158 ust. 3 pkt 1-3, a na wniosek Prezesa Urzędu w razie: 1) niewykonywania obowiązków członka Rady; 2) utraty autorytetu dającego rękojmię prawidłowej realizacji zadań Rady; 3) choroby uniemożliwiającej sprawowanie funkcji członka Rady.\n",
      "Pytanie: Kiedy wygasa członkostwo w Radzie?\n",
      "Oczekiwana odpowiedź: w przypadku upływu kadencji Rady, śmierci członka Rady, jego odwołania albo rezygnacji\n",
      "Wygenerowana odpowiedź: w przypadku upływu kadencji Rady, śmierci członka Rady, jego odwołania albo rezygnacji\n",
      "\n",
      "kontekst: Art. 29. Jeżeli zgodnie z przepisami prawa celnego powiadomienie dłużnika o wysokości długu celnego nie może nastąpić z uwagi na przedawnienie, a istnieje podstawa do obliczenia lub zweryfikowania należności podatkowych, właściwy naczelnik urzędu celnego może określić elementy kalkulacyjne według zasad określonych w przepisach prawa celnego na potrzeby prawidłowego określenia kwoty akcyzy z tytułu importu. Rozdział 6 Zwolnienia\n",
      "Pytanie: Jaki organ dokonuje wymiaru i poboru opłaty celnej dodatkowej?\n",
      "Oczekiwana odpowiedź: naczelnik urzędu celnego\n",
      "Wygenerowana odpowiedź: naczelnik urzędu celnego\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(random_passages)):\n",
    "    print(f\"kontekst: {random_passages[i].passage_text}\")\n",
    "    print(f\"Pytanie: {random_passages[i].qas[0].question}\")\n",
    "    print(f\"Oczekiwana odpowiedź: {random_passages[i].qas[0].answer}\")\n",
    "    print(f\"Wygenerowana odpowiedź: {random_passages_answers[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_exact_matches(random_passages_answers, expected_random_passages_answers, clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6053930461073318"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_f1(random_passages_answers, expected_random_passages_answers, tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ręczna ewaluacja odpowiedzi:\n",
    "\n",
    "| Art nr | zgodność odpowiedzi          | uwagi                                                                                                           |\n",
    "|--------|------------------------------|-----------------------------------------------------------------------------------------------------------------|\n",
    "| 20     | zachowana                    | minimalistyczna odpowiedź modelu                                                                                |\n",
    "| 64     | niezachowana (lecz poprawna) | odpowiedź oczekiwana nie zawiera się w kontekście nasz model odpowiada zgodnie z tekstem dostarczonego przepisu |\n",
    "| 35     | zachowana                    | dokładne dopasowanie                                                                                            |\n",
    "| 31     | niepełna                     | brak informacji o dacie i miejscu wystawienia oraz podpisie pracodawcy                                          |\n",
    "| 36     | niezachowana (lecz poprawna) | odpowiedź oczekiwana nie zawiera się w kontekście nasz model odpowiada zgodnie z tekstem dostarczonego przepisu |\n",
    "| 4      | niezachowana                 | pytanie trudne do odpowiedzi i dla człowieka - brak konkretnych dany                                            |\n",
    "| 90     | zachowana                    | dokładne dopasowanie                                                                                            |\n",
    "| 111    | zachowana                    | minimalistyczna odpowiedź modelu                                                                                |\n",
    "| 159    | zachowana                    | dokładne dopasowanie                                                                                            |\n",
    "| 29     | zachowana                    | dokładne dopasowanie                                                                                            |\n",
    "\n",
    "Wyniki zinterpretuję odpowiadając na pytanie otwarte nr 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytania otwarte\n",
    "\n",
    "#### Czy jakość na zbiorze testowym odpowiada tej dla zbiru walidacyjnego?\n",
    "Niestety nie, metryki *Exact Match* oraz *f1* wypadają znacznie gorzej dla zbioru testowego, niż dla zbioru walidacyjnego. Interpretacja takiego wyniku znajduje się w kolejnym pytaniu, gdzie przyglądam się bliżej małemu wycinkowi przeprowadzonego testu bliżej.\n",
    "\n",
    "#### Jakie wyniki dostarcza model dla pytań testowych? Czy są one satysfakcjonujące?\n",
    "Akurat wyniki dla wylosowanego podzbioru danych testowych wypadły nieco lepiej, niż ocena całego zbioru. Otrzymały one ocenę *Exact Match = 0.4* i *f1 = 0.605* co jest lepszym, od tego dla całego zbioru: *Exact Match = 0.294*, *f1 = 0.521*. Pomijając aspekt szczęśliwego losowania i analizując wykonane zapytania, można dojść do następujących wniosków:\n",
    "- model odpowiada w sposób minimalistyczny, a oczekiwana odpowiedź jest często sformułowana pełnym zdaniem, stąd metryki oceniające zostają zaniżone. W wylosowanym podzbiorze danych testowych, zachowanie to dotyczy 2/10 przypadków.\n",
    "- zbiór danych zawiera oczekiwane odpowiedzi, których ustalenie nie jest możliwe na podstawie dostarczonego kontekstu, stąd model odpowiadający zgodnie z informacją kontekstową oceniony jest negatywnie. Pytania takie powinny zostać oznaczone w zbiorze jako niemożliwe do odpowiedzi, co nie jest wykonane. Zachowanie dotyczy 2/10 przeanalizowanych przypadków.\n",
    "- model potrafi zwrócić niepełną odpowiedź. Przypadek dotyczy 1/10 przypadków\n",
    "- lub wręcz niepoprawną odpowiedź - 1/10 przypadków.\n",
    "\n",
    "Biorąc pod uwagę powyższe uwagi możemy podsumować, że wśród przeanalizownych przypadków 4/10 to dokładne dopasowania, 2/10 są w pełni poprawne, lecz sformułowane lakonicznie, 2/10 niezgodne z oczekiwaną odpowiedzią, lecz w pełni zgodne z kontekstem i poprawne, 1/10 częściowo poprawne i 1/10 niepoprawne.\n",
    "\n",
    "Podsumowując liczby z akapitu powyżej, mamy osiem poprawnych odpowiedzi, jedną częściowo poprawną i jedną niepoprawną, czyli w ponad 80% można być zadowolonym z pracy modelu, co jest wynikiem mnie satysfakcjonującym.  \n",
    "Kiepska ocena modelu uwzględnionymi metrykami ma swoje źródło w danych testowych, które nie są konsekwentne co do odpowiadania równoważnikami zdań, czy też pełnymi zdaniami, oraz zawierającymi odpowiedzi spoza wiedzy zawartej w kontekście bez oznaczenia takich odpowiedzi odpowiednią adnotacją.\n",
    "\n",
    "Co ciekawe, oczekiwałem większego wpływu fleksji języka polskiego na zaniżenie ocen sprawdzonych metryk, ale przypadki wylosowane na to nie wskazują. Przyjrzę się temu zagadnieniu bliżej w ramach odpowiedzi na kolejne pytanie, które dotyka sprawy fleksji języka.\n",
    "\n",
    "#### Dlaczego ekstraktywne odpowiadanie na pytania nie jest właściwe dla językow z bogatą fleksją?\n",
    "W ćwiczeniu używamy abstraktywnego QA, które zaskakująco dobrze radzi sobie z fleksją języka. Postanowiłem przeprowadzić dodatkowy eksperyment i uzupełnić wyliczanie metryki *f1* o dodatkową lematyzację słów pakietem `SpaCy`. W wynikach zawartych poniżej widać, że lematyzacja podniosła wartość *f1* zaledwie do 0.535, czyli o ok 1.5 punktu procentowego względem początkowego wyniku. Pokazuje to jasno, że fleksja nie jest problemem niskich wartości metryk, a ich przyczyna została opisana w odpowiedzi na poprzednie pytanie.\n",
    "\n",
    "Przejdźmy jednak do odpowiedzi na postawione pytanie, które dotyczy modeli QA ekstraktywnych, a nie przebadanego abstraktywnego.  \n",
    "Języki z bogatą fleksją już na poziomie przygotowywania modelu wpływają na jego działanie. Mianowicie model ten musi uwzględnić fleksję zarówno przy tokenizacji, jak i przy budowaniu osadzeń dla wczytanych tokenów. Fleksja prowadzi najpewniej do zwiększenia liczby unikalnych tokenów obsługiwanych przez model, oraz musi zostać uwzględniona w budowie kontekstu dla poszczególnych tokenów, przez co potrzebujemy pojemniejszego modelu i większej liczby zdań w korpusie uczącym, aby dobrze odwzorować zachodzące zależności fleksyjne <- właściwie ta trudność dotyczy zarówno modelu ekstraktywnego, jak i abstraktywnego QA.\n",
    "\n",
    "Dodatkowo, model ekstraktywnego QA cechuje się mniejszą elastycznością, gdyż musi wskazać dokładny początek i koniec odpowiedzi w dostarczonym tekście, natomiast języki bogate fleksyjnie z mojego doświadczenia dają większą swobodę co do budowy zdań i kolejności zawartych w nim części zdania - przykładowo język angielski ma ściślej określoną strukturę zdania, niż np. polski, gdyż jego fleksja jest bardzo uboga, a gdzieś informacja o relacji pomiędzy wyrazami w zdaniu musi zostać zawarta. Skoro zdania języków bogatych fleksyjnie mogą być bardziej zróżnicowane, model mający za zadanie znalezie w tekście początku i końca odpowiedzi ma utrudnione zadanie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dodatkowe wyniki uzyskane przy realizacji zadania\n",
    "\n",
    "Poniżej dodatkowy eksperyment z lematyzacją słów przed zastosowniem metryki *f1*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pl-core-news-lg==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/pl_core_news_lg-3.8.0/pl_core_news_lg-3.8.0-py3-none-any.whl (573.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m573.7/573.7 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:05\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pl-core-news-lg\n",
      "Successfully installed pl-core-news-lg-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('pl_core_news_lg')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download pl_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"pl_core_news_lg\")\n",
    "\n",
    "def tokenize_and_lemmatize(text):\n",
    "    tokens = tokenize(text)\n",
    "    text = \" \".join(tokens)\n",
    "\n",
    "    doc = nlp(text)\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5351370990492306"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_f1(lr5e_5_b16_t5_base_answers, expected_answers, tokenize_and_lemmatize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obliczyłem z ciekawości też wydajności pozostałych modeli na danych testowych. Okazało się, że model o najmniejszej stałej uczącej daje najlepsze wyniki, lecz skoro nie był najlepszy dla danych walidacyjnych, to szanując zasady przeprowadzonych badań nie użyłem go w zasadniczej części zbierania wynikow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| lr   | #batchy | eval em | eval f1 |\n",
    "|------|---------|---------|---------|\n",
    "| 2e-5 | 8       | 0.305   | 0.535   |\n",
    "| 3e-5 | 12      | 0.287   | 0.5257  |\n",
    "| 4e-5 | 16      | 0.293   | 0.5260  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na końcu mała funkcja pomocnicza, która zapisuje i wczytuje obliczone odpowiedzi. Szczególnie istotna, gdyż wyznaczanie odpowiedzi przez model w sposób czysto sekwencyjny zajmowało ok 2h dla danych walidacyjnych i ok 0.5h dla zbioru testowego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def store_answers_in_json(answers, file_path):\n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(answers, f)\n",
    "\n",
    "def load_answers_from_json(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_answers_in_json(lr5e_5_b16_t5_base_val_answers, \"lr5e_5_b16_t5_base_val_answers.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
