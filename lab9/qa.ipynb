{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marcin Wardyński  \n",
    "czwartek, 8:00\n",
    "\n",
    "## Lab 9: Kontekstowe odpowiadanie na pytania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dla przygotowania danych treningowych i walidacyjnych dla modelu trzeba przerobić istniejące pliki json. Format nowych plików musi zawierać elementy:\n",
    "- `id`\n",
    "- `context`\n",
    "- `question`\n",
    "- `answers` z polem `text` zawierającym w postaci listy poprawne odpowiedzi\n",
    "\n",
    "Poniższa funkcja dokonuje oczekiwanego przekształcenia na zbiorze PoQuAD: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def convert_data(data):\n",
    "    results = []\n",
    "    i = 0\n",
    "    for article in data.get(\"data\", []):\n",
    "        for paragraph in article.get(\"paragraphs\", []):\n",
    "            context = paragraph[\"context\"]\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                question = qa[\"question\"]\n",
    "                answers = qa['answers'] if 'answers' in qa.keys() else qa['plausible_answers']\n",
    "                for answer in answers:\n",
    "                    i += 1\n",
    "                    results.append({\n",
    "                        \"id\": i,\n",
    "                        \"context\": context,\n",
    "                        \"question\": question,\n",
    "                        \"answers\": {\n",
    "                            \"text\": [answer[\"generative_answer\"]]\n",
    "                        }\n",
    "                    })\n",
    "    return results\n",
    "\n",
    "\n",
    "def convert_format(input_filepath, output_filepath):\n",
    "    with open(input_filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    output_data = convert_data(data)\n",
    "    output_wrapped_data = {\"version\": \"0.1.0\", \"data\": output_data}\n",
    "\n",
    "    with open(output_filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(output_wrapped_data, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wywołuję powyższą funkcję i zapisuję przekształcone dane w plikach `poquad-conv-train.json` oraz `poquad-conv-dev.json` dla zbiorów treningowego i walidacyjnego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_format(\"poquad-train.json\", \"poquad-conv-train.json\")\n",
    "convert_format(\"poquad-dev.json\", \"poquad-conv-dev.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python run_seq2seq_qa.py \\\n",
    "  --model_name_or_path allegro/plt5-base \\\n",
    "  --dataset_name clarin-pl/poquad \\\n",
    "  --context_column context \\\n",
    "  --question_column question \\\n",
    "  --answer_column answers \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --per_device_train_batch_size 12 \\\n",
    "  --learning_rate 3e-5 \\\n",
    "  --num_train_epochs 3 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --output_dir ../../../../model_poquad_extr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python run_seq2seq_qa.py \\\n",
    "  --model_name_or_path allegro/plt5-base \\\n",
    "  --train_file /Users/mwardynski/Documents/ds/_semestr_9/przetwarzanie_jezyka_naturalnego/labs/lab9/poquad-conv-train.json \\\n",
    "  --validation_file /Users/mwardynski/Documents/ds/_semestr_9/przetwarzanie_jezyka_naturalnego/labs/lab9/poquad-conv-dev.json \\\n",
    "  --context_column context \\\n",
    "  --question_column question \\\n",
    "  --answer_column answers \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --per_device_train_batch_size 12 \\\n",
    "  --learning_rate 3e-5 \\\n",
    "  --num_train_epochs 3 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --output_dir ../../../../model_poquad_abstr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python run_seq2seq_qa.py \\\n",
    "  --model_name_or_path allegro/plt5-base \\\n",
    "  --train_file /Users/mwardynski/Documents/ds/_semestr_9/przetwarzanie_jezyka_naturalnego/labs/lab9/poquad-conv-train.json \\\n",
    "  --validation_file /Users/mwardynski/Documents/ds/_semestr_9/przetwarzanie_jezyka_naturalnego/labs/lab9/poquad-conv-dev.json \\\n",
    "  --context_column context \\\n",
    "  --question_column question \\\n",
    "  --answer_column answers \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --per_device_train_batch_size 16 \\\n",
    "  --learning_rate 2e-5 \\\n",
    "  --num_train_epochs 3 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --output_dir ../../../../model_poquad_t5_base_2e-5_b16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "***** train metrics *****\n",
    "  epoch                    =                3.0\n",
    "  total_flos               =         86256742GF\n",
    "  train_loss               =             1.6838\n",
    "  train_runtime            = 2 days, 0:55:20.98\n",
    "  train_samples            =              56618\n",
    "  train_samples_per_second =              0.964\n",
    "  train_steps_per_second   =               0.06\n",
    "\n",
    "***** eval metrics *****\n",
    "  epoch                   =        3.0\n",
    "  eval_loss               =     0.9395\n",
    "  eval_runtime            = 0:06:16.18\n",
    "  eval_samples            =       7539\n",
    "  eval_samples_per_second =      20.04\n",
    "  eval_steps_per_second   =      2.507\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python run_seq2seq_qa.py \\\n",
    "  --model_name_or_path allegro/plt5-base \\\n",
    "  --train_file /Users/mwardynski/Documents/ds/_semestr_9/przetwarzanie_jezyka_naturalnego/labs/lab9/poquad-conv-train.json \\\n",
    "  --validation_file /Users/mwardynski/Documents/ds/_semestr_9/przetwarzanie_jezyka_naturalnego/labs/lab9/poquad-conv-dev-short.json \\\n",
    "  --context_column context \\\n",
    "  --question_column question \\\n",
    "  --answer_column answers \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --per_device_train_batch_size 16 \\\n",
    "  --learning_rate 4e-5 \\\n",
    "  --num_train_epochs 3 \\\n",
    "  --eval_strategy steps \\\n",
    "  --eval_steps 1 \\\n",
    "  --save_steps 500 \\\n",
    "  --metric_for_best_model f1 \\\n",
    "  --predict_with_generate True \\\n",
    "  --greater_is_better True \\\n",
    "  --load_best_model_at_end True \\\n",
    "  --save_total_limit 3 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --output_dir ../../../../model_poquad_t5_base_f1_4e-5_b16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python run_seq2seq_qa.py \\\n",
    "  --model_name_or_path allegro/plt5-base \\\n",
    "  --train_file /Users/mwardynski/Documents/ds/_semestr_9/przetwarzanie_jezyka_naturalnego/labs/lab9/poquad-conv-train.json \\\n",
    "  --validation_file /Users/mwardynski/Documents/ds/_semestr_9/przetwarzanie_jezyka_naturalnego/labs/lab9/poquad-conv-dev.json \\\n",
    "  --context_column context \\\n",
    "  --question_column question \\\n",
    "  --answer_column answers \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --per_device_train_batch_size 16 \\\n",
    "  --learning_rate 4e-5 \\\n",
    "  --num_train_epochs 3 \\\n",
    "  --eval_strategy steps \\\n",
    "  --eval_steps 500 \\\n",
    "  --save_steps 500 \\\n",
    "  --predict_with_generate True \\\n",
    "  --metric_for_best_model f1 \\\n",
    "  --greater_is_better True \\\n",
    "  --load_best_model_at_end True \\\n",
    "  --save_total_limit 3 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --output_dir ../../../../model_poquad_t5_base_f1_4e-5_b16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***** train metrics *****\n",
    "  epoch                    =        3.0\n",
    "  total_flos               = 86256742GF\n",
    "  train_loss               =     0.2784\n",
    "  train_runtime            = 8:25:03.53\n",
    "  train_samples            =      56618\n",
    "  train_samples_per_second =      5.605\n",
    "  train_steps_per_second   =       0.35\n",
    "\n",
    "***** eval metrics *****\n",
    "  epoch                   =        3.0\n",
    "  eval_exact_match        =    51.3881\n",
    "  eval_f1                 =    67.2528\n",
    "  eval_loss               =     0.8218\n",
    "  eval_runtime            = 0:39:08.28\n",
    "  eval_samples            =       7539\n",
    "  eval_samples_per_second =       3.21\n",
    "  eval_steps_per_second   =      0.402"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python run_seq2seq_qa.py \\\n",
    "  --model_name_or_path allegro/plt5-base \\\n",
    "  --train_file /Users/mwardynski/Documents/ds/_semestr_9/przetwarzanie_jezyka_naturalnego/labs/lab9/poquad-conv-train.json \\\n",
    "  --validation_file /Users/mwardynski/Documents/ds/_semestr_9/przetwarzanie_jezyka_naturalnego/labs/lab9/poquad-conv-dev.json \\\n",
    "  --context_column context \\\n",
    "  --question_column question \\\n",
    "  --answer_column answers \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --per_device_train_batch_size 8 \\\n",
    "  --learning_rate 2e-5 \\\n",
    "  --num_train_epochs 3 \\\n",
    "  --eval_strategy steps \\\n",
    "  --eval_steps 1000 \\\n",
    "  --save_steps 1000 \\\n",
    "  --predict_with_generate True \\\n",
    "  --metric_for_best_model f1 \\\n",
    "  --greater_is_better True \\\n",
    "  --load_best_model_at_end True \\\n",
    "  --save_total_limit 3 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --output_dir ../../../../model_poquad_t5_base_f1_2e-5_b8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***** train metrics *****\n",
    "  epoch                    =         3.0\n",
    "  total_flos               =  86256742GF\n",
    "  train_loss               =      0.3401\n",
    "  train_runtime            = 13:55:17.07\n",
    "  train_samples            =       56618\n",
    "  train_samples_per_second =       3.389\n",
    "  train_steps_per_second   =       0.424\n",
    "\n",
    "***** eval metrics *****\n",
    "  epoch                   =        3.0\n",
    "  eval_exact_match        =    51.5297\n",
    "  eval_f1                 =    67.6208\n",
    "  eval_loss               =     0.8603\n",
    "  eval_runtime            = 0:41:46.65\n",
    "  eval_samples            =       7539\n",
    "  eval_samples_per_second =      3.008\n",
    "  eval_steps_per_second   =      0.376"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python run_seq2seq_qa.py \\\n",
    "  --model_name_or_path allegro/plt5-base \\\n",
    "  --train_file /Users/mwardynski/Documents/ds/_semestr_9/przetwarzanie_jezyka_naturalnego/labs/lab9/poquad-conv-train.json \\\n",
    "  --validation_file /Users/mwardynski/Documents/ds/_semestr_9/przetwarzanie_jezyka_naturalnego/labs/lab9/poquad-conv-dev.json \\\n",
    "  --context_column context \\\n",
    "  --question_column question \\\n",
    "  --answer_column answers \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --per_device_train_batch_size 16 \\\n",
    "  --learning_rate 5e-5 \\\n",
    "  --num_train_epochs 3 \\\n",
    "  --eval_strategy steps \\\n",
    "  --eval_steps 500 \\\n",
    "  --save_steps 500 \\\n",
    "  --predict_with_generate True \\\n",
    "  --metric_for_best_model f1 \\\n",
    "  --greater_is_better True \\\n",
    "  --load_best_model_at_end True \\\n",
    "  --save_total_limit 3 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --output_dir ../../../../model_poquad_t5_base_f1_5e-5_b16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "***** train metrics *****\n",
    "  epoch                    =         3.0\n",
    "  total_flos               =  86256742GF\n",
    "  train_loss               =      0.3226\n",
    "  train_runtime            = 13:12:21.12\n",
    "  train_samples            =       56618\n",
    "  train_samples_per_second =       3.573\n",
    "  train_steps_per_second   =       0.223\n",
    "\n",
    "***** eval metrics *****\n",
    "  epoch                   =        3.0\n",
    "  eval_exact_match        =    52.3513\n",
    "  eval_f1                 =    68.3034\n",
    "  eval_loss               =     0.8013\n",
    "  eval_runtime            = 0:39:11.65\n",
    "  eval_samples            =       7539\n",
    "  eval_samples_per_second =      3.206\n",
    "  eval_steps_per_second   =      0.401\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python run_seq2seq_qa.py \\\n",
    "  --model_name_or_path allegro/plt5-base \\\n",
    "  --train_file /Users/mwardynski/Documents/ds/_semestr_9/przetwarzanie_jezyka_naturalnego/labs/lab9/poquad-conv-train.json \\\n",
    "  --validation_file /Users/mwardynski/Documents/ds/_semestr_9/przetwarzanie_jezyka_naturalnego/labs/lab9/poquad-conv-dev.json \\\n",
    "  --context_column context \\\n",
    "  --question_column question \\\n",
    "  --answer_column answers \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --per_device_train_batch_size 12 \\\n",
    "  --learning_rate 3e-5 \\\n",
    "  --num_train_epochs 3 \\\n",
    "  --eval_strategy steps \\\n",
    "  --eval_steps 500 \\\n",
    "  --save_steps 500 \\\n",
    "  --predict_with_generate True \\\n",
    "  --metric_for_best_model f1 \\\n",
    "  --greater_is_better True \\\n",
    "  --load_best_model_at_end True \\\n",
    "  --save_total_limit 3 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --output_dir ../../../../model_poquad_t5_base_f1_3e-5_b12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "***** train metrics *****\n",
    "  epoch                    =               3.0\n",
    "  total_flos               =        86256742GF\n",
    "  train_loss               =            1.2486\n",
    "  train_runtime            = 1 day, 4:45:16.29\n",
    "  train_samples            =             56618\n",
    "  train_samples_per_second =             1.641\n",
    "  train_steps_per_second   =             0.137\n",
    "\n",
    "***** eval metrics *****\n",
    "  epoch                   =        3.0\n",
    "  eval_exact_match        =    50.8499\n",
    "  eval_f1                 =    66.8706\n",
    "  eval_loss               =     0.8446\n",
    "  eval_runtime            = 0:47:32.25\n",
    "  eval_samples            =       7539\n",
    "  eval_samples_per_second =      2.643\n",
    "  eval_steps_per_second   =      0.331\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "ready_model_name = \"apohllo/plt5-base-poquad\"\n",
    "ready_model_tokenizer = AutoTokenizer.from_pretrained(ready_model_name)\n",
    "ready_model = AutoModelForSeq2SeqLM.from_pretrained(ready_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Czy żołnierz, który dopuszcza się czynnej napaści na przełożonego podlega karze pozbawienia wolności?\n",
      "Answer: tak\n"
     ]
    }
   ],
   "source": [
    "context = \"Art. 345. § 1. Żołnierz, który dopuszcza się czynnej napaści na przełożonego, podlega karze aresztu wojskowego albo pozbawienia wolności do lat 3. § 2. Jeżeli sprawca dopuszcza się czynnej napaści w związku z pełnieniem przez przełożonego obowiązków służbowych albo wspólnie z innymi żołnierzami lub w obecności zebranych żołnierzy, podlega karze pozbawienia wolności od 6 miesięcy do lat 8. § 3. Jeżeli sprawca czynu określonego w § 1 lub 2 używa broni, noża lub innego podobnie niebezpiecznego przedmiotu, podlega karze pozbawienia wolności od roku do lat 10. § 4. Karze przewidzianej w § 3 podlega sprawca czynu określonego w § 1 lub 2, jeżeli jego następstwem jest skutek określony w art. 156 lub 157 § 1.\"\n",
    "question = \"Czy żołnierz, który dopuszcza się czynnej napaści na przełożonego podlega karze pozbawienia wolności?\"\n",
    "\n",
    "input_text = f\"question: {question} context: {context}\"\n",
    "\n",
    "inputs = ready_model_tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "outputs = ready_model.generate(inputs[\"input_ids\"], max_length=50, num_beams=5, early_stopping=True)\n",
    "\n",
    "answer = ready_model_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do fine-tuningu swojego modelu użyłem nieznacznie zmienionego skryptu `run_seq2seq_qa.py` z repozytorium `transformers`, który został wywołany z następującymi parametrami:\n",
    "```\n",
    "  --model_name_or_path allegro/plt5-base \\\n",
    "  --train_file ../../../..poquad-conv-train.json \\\n",
    "  --validation_file ../../../..poquad-conv-dev.json \\\n",
    "  --context_column context \\\n",
    "  --question_column question \\\n",
    "  --answer_column answers \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --per_device_train_batch_size X \\\n",
    "  --learning_rate Ye-5 \\\n",
    "  --num_train_epochs 3 \\\n",
    "  --eval_strategy steps \\\n",
    "  --eval_steps 500 \\\n",
    "  --save_steps 500 \\\n",
    "  --predict_with_generate True \\\n",
    "  --metric_for_best_model f1 \\\n",
    "  --greater_is_better True \\\n",
    "  --load_best_model_at_end True \\\n",
    "  --save_total_limit 3 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --output_dir ../../../../model_poquad_t5_base_f1_3e-5_b12\n",
    "```\n",
    "\n",
    "Jak widzimy powyżęj, jako modelu pretrenowanego używam `allegro/plt5-base`, gdyż moja konfiguracja komputera nie pozwala mi na wykorzystanie pojemniejszego modelu. Do treningu przekazuję uprzednio przygotowane pliki ze zbiorem danych PoQuAD oraz wskazuje nazwy odpowiednich kolumn. Zlecam wykonanie treningu i ewaluacji z wielkością batcha i krokiem treningu zdefiniowanym dla każdego uruchomienia skryptu z innymi wartościami. Trening ma trwać zalecane trzy epoki, a wybrana strategia ewaluacji `f1` powinna zostać uruchomiona co 500 kroków. Finalnie powinien zostać zaladowany model o najlepszym wyniku ewaluacji. Parametry `max_seq_length` oraz `doc_stride` otrzymały wartości odpowiednio 384 i 128. (Chociaż pierwsza z nich mogłaby w zasadzie otrzymać wartość 512)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wyniki skyptu dla wybranych parametrów treningowych:\n",
    "\n",
    "| lr       | #batchy | eval em   | eval f1   |\n",
    "|----------|---------|-----------|-----------|\n",
    "| 2e-5     | 8       | 51.53     | 67.62     |\n",
    "| 3e-5     | 12      | 50.84     | 66.87     |\n",
    "| 4e-5     | 16      | 51.39     | 67.25     |\n",
    "| **5e-5** | **16**  | **52.35** | **68.30** |\n",
    "\n",
    "Jak widzimy, dla danych walidacyjnych zbioru PoQuAD najlepiej wypada model o stałej uczącej `5e-5` i rozmiarze batch-a `16` i to zarówno dla metryki *Exact Match*, jak i *f1*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skoro mamy już wytrenowany model wraz z jego wstępną ewaluacją, skupmy się na danych testowych. Poniższy zestaw funkcji buduje struktury słownikowe, które ułatwiają nawigowanie po danych testowych zawierających wskazane pytania prawne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "NO_ANS = \"no_ans\"\n",
    "\n",
    "class QA:\n",
    "    def __init__(self, question_id, question, answer):\n",
    "        self.question_id = question_id\n",
    "        self.question = question\n",
    "        self.answer = answer\n",
    "\n",
    "class Entry:\n",
    "    def __init__(self, passage_id, passage_text, qas):\n",
    "        self.passage_id = passage_id\n",
    "        self.passage_text = passage_text\n",
    "        self.qas = qas\n",
    "\n",
    "def init_qas_with_answers(filepath):\n",
    "    qa_dict = {}\n",
    "    with open(filepath, \"r\") as file:\n",
    "        for line in file:\n",
    "            record = json.loads(line.strip())\n",
    "            \n",
    "            if \"score\" in record and record[\"score\"] == \"1\"\\\n",
    "                    and \"question-id\" in record and \"answer\" in record:\n",
    "                qa = QA(record[\"question-id\"], None, record[\"answer\"])\n",
    "                qa_dict[record[\"question-id\"]] = qa\n",
    "    return qa_dict\n",
    "\n",
    "def match_questions_to_answers(filepath, qa_dict):\n",
    "    q_wo_a = []\n",
    "    with open(filepath, \"r\") as file:\n",
    "        for line in file:\n",
    "            record = json.loads(line.strip())\n",
    "            \n",
    "            if \"text\" in record and \"_id\" in record:\n",
    "                if record[\"_id\"] in qa_dict.keys():\n",
    "                    qa = qa_dict[record[\"_id\"]]\n",
    "                    qa.question = record[\"text\"]\n",
    "                else:\n",
    "                    qa_dict[NO_ANS].append(record[\"text\"])\n",
    "\n",
    "\n",
    "def organize_question_to_context_relations(filepath, qa_dict):\n",
    "    qc_dict = {}\n",
    "    with open(filepath, \"r\") as file:\n",
    "        for line in file:\n",
    "            record = json.loads(line.strip())\n",
    "            \n",
    "            if \"score\" in record and record[\"score\"] == \"1\"\\\n",
    "                    and \"passage-id\" in record\\\n",
    "                    and \"question-id\" in record and record[\"question-id\"] in qa_dict.keys():\n",
    "                if record[\"passage-id\"] not in qc_dict.keys():\n",
    "                    qc_dict[record[\"passage-id\"]] = []\n",
    "                qc_dict[record[\"passage-id\"]].append(record[\"question-id\"])\n",
    "    return qc_dict\n",
    "\n",
    "def load_passages(filepath, qc_dict, qa_dict):\n",
    "    entries = []\n",
    "    with open(filepath, \"r\") as file:\n",
    "        for line in file:\n",
    "            record = json.loads(line.strip())\n",
    "            \n",
    "            if \"text\" in record and \"_id\" in record and record[\"_id\"] in qc_dict.keys():\n",
    "                qa_ids = qc_dict[record[\"_id\"]]\n",
    "                qas = []\n",
    "                for qa_id in qa_ids:\n",
    "                    qas.append(qa_dict[qa_id])\n",
    "                entries.append(Entry(record[\"_id\"], record[\"text\"], qas))\n",
    "    return entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po uruchomieniu tych fynkcji i przekazaniu odpowiednich plików wejściowych otrzymujemy następujące struktury:\n",
    "`qa_dict` - słownik łączący pytania z odpowiedziami\n",
    "`qc_dict` - słownik łączący pytania z kontekstem\n",
    "`test_passages` - lista z kontekstem oraz połączonymi z nim pytaniami i odpowiedziami\n",
    "\n",
    "Tak przygotowanych struktur będą wykorzystywać, aby zewaluować jakość modelu dla danych testowych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_dict = init_qas_with_answers(\"simple-legal-questions-pl-main/answers.jl\")\n",
    "\n",
    "qa_dict[NO_ANS] = []\n",
    "match_questions_to_answers(\"simple-legal-questions-pl-main/questions.jl\", qa_dict)\n",
    "\n",
    "qc_dict = organize_question_to_context_relations(\"simple-legal-questions-pl-main/relevant.jl\", qa_dict)\n",
    "test_passages = load_passages(\"simple-legal-questions-pl-main/passages.jl\", qc_dict, qa_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dodatkowo napisałem funkcję przekształcającą dane zbioru PoQuAD do reprezentacji odpowiadającej danym testowym. Napisałem ją, gdyż chciałbym dodatkowo policzyć metryki dla zbioru walidacyjnego dokładnie tymi samymi funkcjami, co dla zbioru testowego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_poquad_data(filepath):\n",
    "    val_entries = []\n",
    "    with open(filepath, \"r\") as file:\n",
    "        json_content = json.load(file)\n",
    "        for data in json_content['data']:\n",
    "            for paragraph in data['paragraphs']:\n",
    "                qa_list = []\n",
    "                for qa in paragraph['qas']:\n",
    "                    answers = qa['answers'] if 'answers' in qa.keys() else qa['plausible_answers']\n",
    "                    for answer in answers:\n",
    "                        qa_obj = QA(None, qa['question'], answer['generative_answer'])\n",
    "                        qa_list.append(qa_obj)\n",
    "                    \n",
    "                entry = Entry(None, paragraph['context'], qa_list)\n",
    "                val_entries.append(entry)\n",
    "    return val_entries\n",
    "\n",
    "val_passages = convert_poquad_data(\"poquad-dev.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcja `exec_passages` przechodzi po kontekstach w podanym zbiorze i przekazuje je do modelu wraz z pytaniami i odpowiedziami na nie. Wyjściowo otrzymujemy listę odpowiedzi modelu oraz oczekiwanych odpowiedzi generatywnych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def exec_passages(model, tokenizer, passages):\n",
    "    answers = []\n",
    "    expected_answers = []\n",
    "    for passage in tqdm(passages):\n",
    "        for qa in passage.qas:\n",
    "            input_text = f\"question: {qa.question} context: {passage.passage_text}\"\n",
    "            inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "            outputs = model.generate(inputs[\"input_ids\"], max_length=100, num_beams=5, early_stopping=True)\n",
    "\n",
    "            answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            answers.append(answer)\n",
    "            expected_answers.append(qa.answer)\n",
    "    return answers, expected_answers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz wystarczy wczytać odpowiedni model i wyznaczyć jego odpowiedzi na pytania ze zbioru poprzez wywolanie powyższej funkcji. Wykonuję te działania dla każdego z wytrenowanych modeli następującymi wywołaniami:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "my_t5_base_model_name = \"./model_poquad_abstr\"\n",
    "my_t5_base_model_tokenizer = AutoTokenizer.from_pretrained(my_t5_base_model_name)\n",
    "my_t5_base_model = AutoModelForSeq2SeqLM.from_pretrained(my_t5_base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 557/557 [11:03<00:00,  1.19s/it]\n"
     ]
    }
   ],
   "source": [
    "my_answers, expected_answers = exec_passages(my_t5_base_model, my_t5_base_model_tokenizer, test_passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1453/1453 [2:33:27<00:00,  6.34s/it]  \n"
     ]
    }
   ],
   "source": [
    "my_val_answers, val_expected_answers = exec_passages(my_t5_base_model, my_t5_base_model_tokenizer, val_passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "lr2e_5_b16_t5_base_model_name = \"./model_poquad_t5_base_2e-5_b16\"\n",
    "lr2e_5_b16_t5_base_model_tokenizer = AutoTokenizer.from_pretrained(lr2e_5_b16_t5_base_model_name)\n",
    "lr2e_5_b16_t5_base_model = AutoModelForSeq2SeqLM.from_pretrained(lr2e_5_b16_t5_base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 557/557 [13:39<00:00,  1.47s/it]\n"
     ]
    }
   ],
   "source": [
    "lr2e_5_b16_t5_base_answers, expected_answers = exec_passages(lr2e_5_b16_t5_base_model, lr2e_5_b16_t5_base_model_tokenizer, test_passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "lr4e_5_b16_t5_base_model_name = \"./model_poquad_t5_base_f1_4e-5_b16\"\n",
    "lr4e_5_b16_t5_base_model_tokenizer = AutoTokenizer.from_pretrained(lr4e_5_b16_t5_base_model_name)\n",
    "lr4e_5_b16_t5_base_model = AutoModelForSeq2SeqLM.from_pretrained(lr4e_5_b16_t5_base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 557/557 [15:34<00:00,  1.68s/it]\n"
     ]
    }
   ],
   "source": [
    "lr4e_5_b16_t5_base_answers, expected_answers = exec_passages(lr4e_5_b16_t5_base_model, lr4e_5_b16_t5_base_model_tokenizer, test_passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1453/1453 [2:07:02<00:00,  5.25s/it] \n"
     ]
    }
   ],
   "source": [
    "lr4e_5_b16_t5_base_val_answers, expected_val_answers = exec_passages(lr4e_5_b16_t5_base_model, lr4e_5_b16_t5_base_model_tokenizer, val_passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "lr2e_5_b8_t5_base_model_name = \"./model_poquad_t5_base_f1_2e-5_b8\"\n",
    "lr2e_5_b8_t5_base_model_tokenizer = AutoTokenizer.from_pretrained(lr2e_5_b8_t5_base_model_name)\n",
    "lr2e_5_b8_t5_base_model = AutoModelForSeq2SeqLM.from_pretrained(lr2e_5_b8_t5_base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 557/557 [14:14<00:00,  1.53s/it]\n"
     ]
    }
   ],
   "source": [
    "lr2e_5_b8_t5_base_answers, expected_answers = exec_passages(lr2e_5_b8_t5_base_model, lr2e_5_b8_t5_base_model_tokenizer, test_passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1453/1453 [2:14:53<00:00,  5.57s/it] \n"
     ]
    }
   ],
   "source": [
    "lr2e_5_b8_t5_base_val_answers, expected_val_answers = exec_passages(lr2e_5_b8_t5_base_model, lr2e_5_b8_t5_base_model_tokenizer, val_passages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz wystarczy wczytać odpowiedni model i wyznaczyć jego odpowiedzi na pytania ze zbioru poprzez wywołanie powyższej funkcji. W pierwszej kolejności wykonuję te działania dla każdego z wytrenowanych modeli na zbiorze walidacyjnym następującymi wywołaniami:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "lr5e_5_b16_t5_base_model_name = \"./model_poquad_t5_base_f1_5e-5_b16\"\n",
    "lr5e_5_b16_t5_base_model_tokenizer = AutoTokenizer.from_pretrained(lr5e_5_b16_t5_base_model_name)\n",
    "lr5e_5_b16_t5_base_model = AutoModelForSeq2SeqLM.from_pretrained(lr5e_5_b16_t5_base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1453/1453 [2:08:29<00:00,  5.31s/it] \n"
     ]
    }
   ],
   "source": [
    "lr5e_5_b16_t5_base_val_answers, expected_val_answers = exec_passages(lr5e_5_b16_t5_base_model, lr5e_5_b16_t5_base_model_tokenizer, val_passages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przedstawiam przykład tylko dla jednego z modeli, reszta została użyta w analogiczny sposób.\n",
    "\n",
    "Następnie definiuję funkcje do obliczania miar \"Exact Match\" oraz \"f1\".\n",
    "\n",
    "Funkcja obliczająca *Exact Match* zawiera mały etap wstępnego przetworzenia tekstów na wejściu, który to usuwa skrajne spacje, znaki interpunkcyjne i zmniejsza czcionkę wszystkich słów, tak żeby lepiej odkrywać dokładne dopasowania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def clean_text(s):\n",
    "    s = s.strip()\n",
    "    s = s.lower()\n",
    "    s = ''.join(c for c in s if c not in string.punctuation)\n",
    "    return s\n",
    "\n",
    "\n",
    "def calculate_exact_matches(answers, expected_answers, clean_fun):\n",
    "    matches = 0\n",
    "    for s1, s2 in zip(answers, expected_answers):\n",
    "        if clean_fun(s1) == clean_fun(s2):\n",
    "            matches += 1\n",
    "    return matches/len(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "lr3e_5_b12_t5_base_model_name = \"./model_poquad_t5_base_f1_3e-5_b12\"\n",
    "lr3e_5_b12_t5_base_model_tokenizer = AutoTokenizer.from_pretrained(lr3e_5_b12_t5_base_model_name)\n",
    "lr3e_5_b12_t5_base_model = AutoModelForSeq2SeqLM.from_pretrained(lr3e_5_b12_t5_base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 557/557 [13:42<00:00,  1.48s/it]\n"
     ]
    }
   ],
   "source": [
    "lr3e_5_b12_t5_base_answers, expected_answers = exec_passages(lr3e_5_b12_t5_base_model, lr3e_5_b12_t5_base_model_tokenizer, test_passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1453/1453 [2:00:22<00:00,  4.97s/it] \n"
     ]
    }
   ],
   "source": [
    "lr3e_5_b12_t5_base_val_answers, expected_val_answers = exec_passages(lr3e_5_b12_t5_base_model, lr3e_5_b12_t5_base_model_tokenizer, val_passages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcja obliczająca *Exact Match* zawiera mały etap wstępnego przetworzenia tekstów na wejściu, który to usuwa skrajne spacje, znaki interpunkcyjne i zmniejsza czcionkę wszystkich słów, tak żeby lepiej odkrywać dokładne dopasowania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def clean_text(s):\n",
    "    s = s.strip()\n",
    "    s = s.lower()\n",
    "    s = ''.join(c for c in s if c not in string.punctuation)\n",
    "    return s\n",
    "\n",
    "\n",
    "def calculate_exact_matches(answers, expected_answers, clean_fun):\n",
    "    matches = 0\n",
    "    for s1, s2 in zip(answers, expected_answers):\n",
    "        if clean_fun(s1) == clean_fun(s2):\n",
    "            matches += 1\n",
    "    return matches/len(answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ostatecznie funkcja mierząca wartości TP, FP, FN i wyznaczająca na ich podstawie metryki `precision` i `recall` aby zwrócić opierającą się o nie metrykę `f1`. Poza dwoma odpowiedziami funkcja przyjmuje funkcję tokenizującą, która przeprowadza czyszczenie tekstu takie samo, jak w przypadku *Exact Match*, i dodatkowo tworzy tokeny 1:1 ze słów zdania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    text = clean_text(text)\n",
    "    tokens = re.split(r\"[^\\w]+\", text)\n",
    "    return tokens\n",
    "\n",
    "def compute_single_f1(tokens1, tokens2):\n",
    "    TP = sum((tokens1 & tokens2).values())\n",
    "    FP = sum((tokens1 - tokens2).values())\n",
    "    FN = sum((tokens2 - tokens1).values())\n",
    "\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return f1\n",
    "\n",
    "def compute_f1(answers, expected_answers, tokenize_fun):\n",
    "\n",
    "    f1_scores = []\n",
    "    for s1, s2 in zip(answers, expected_answers):\n",
    "        s1_t = Counter(tokenize_fun(s1))\n",
    "        s2_t = Counter(tokenize_fun(s2))\n",
    "        f1 = compute_single_f1(s1_t, s2_t)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    return sum(f1_scores)/len(f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Następnie wywołuję obydwie funkcje mierzące miary *Exact Match* i *f1* na zbiorze walidacyjnym.  \n",
    "Powtarzam pomiary tych metryk, chociaż zostały mi już one zwrócone podczas treningu, gdyż chcę wykorzystać te same funkcje, których będą wykorzystywal później dla zbioru testowego. Chcę, żeby porównanie wyników dla zbioru walidacyjnego oraz testowego miało większy sens.\n",
    "\n",
    "Jak widzimy powyżej, moje implementacje funkcji obliczających te metryki dodatkowo czyszczą wprowadzony tekst, a miara *f1* jest obliczana w sposób mikro (czyli liczymy *f1* dla każdego przypadku z osobna i uśredniamy już wyliczony zbiór wartości *f1*, żeby uzyskać wartość średnią dla zbioru).  \n",
    "Co do funkcji użytych w skrypcie `run_seq2seq_qa.py`, to nie mam pewności w jaki sposób miary te zostały obliczone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.553399433427762"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_exact_matches(lr5e_5_b16_t5_base_val_answers, expected_val_answers, clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7320584815022007"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_f1(lr5e_5_b16_t5_base_val_answers, expected_val_answers, tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wyniki dla wywołań wszystkich moich funkcji ewaluacyjnych na zbiorze walidacyjnym zamieszczam poniżej:\n",
    "\n",
    "| lr       | #batchy | eval em   | eval f1   |\n",
    "|----------|---------|-----------|-----------|\n",
    "| 2e-5     | 8       | 0.544     | 0.723     |\n",
    "| 3e-5     | 12      | 0.533     | 0.713     |\n",
    "| 4e-5     | 16      | 0.538     | 0.718     |\n",
    "| **5e-5** | **16**  | **0.553** | **0.732** |\n",
    "\n",
    "Widzimy, że faktycznie wyliczone wartości różnią się od tych zwróconych przez skypt treningowy i są odrobinę wyższe. Ma to najpewniej swoje źródło w dodatkowym, minimalnym przygotowaniu danych do porównania, które stosuję."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2926829268292683"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_exact_matches(lr4e_5_b16_t5_base_answers, expected_answers, clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3048780487804878"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_exact_matches(lr2e_5_b8_t5_base_answers, expected_answers, clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29442508710801396"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_exact_matches(lr5e_5_b16_t5_base_answers, expected_answers, clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2874564459930314"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_exact_matches(lr3e_5_b12_t5_base_answers, expected_answers, clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5330028328611898"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_exact_matches(lr3e_5_b12_t5_base_val_answers, expected_val_answers, clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ostatecznie funkcja mierząca wartości TP, FP, FN i wyznaczająca na ich podstawie metryki `precision` i `recall` aby zwrócić opierającą się o nie metrykę `f1`. Poza dwoma odpowiedziami funkcja przyjmuje funkcję tokenizującą, która przeprowadza czyszczenie tekstu takie samo, jak w przypadku *Exact Match*, i dodatkowo tworzy tokeny z już dalej nieprzetworzonych słów zdania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    text = clean_text(text)\n",
    "    tokens = re.split(r\"[^\\w]+\", text)\n",
    "    return tokens\n",
    "\n",
    "def compute_single_f1(tokens1, tokens2):\n",
    "    TP = sum((tokens1 & tokens2).values())\n",
    "    FP = sum((tokens1 - tokens2).values())\n",
    "    FN = sum((tokens2 - tokens1).values())\n",
    "\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return f1\n",
    "\n",
    "def compute_f1(answers, expected_answers, tokenize_fun):\n",
    "\n",
    "    f1_scores = []\n",
    "    for s1, s2 in zip(answers, expected_answers):\n",
    "        s1_t = Counter(tokenize_fun(s1))\n",
    "        s2_t = Counter(tokenize_fun(s2))\n",
    "        f1 = compute_single_f1(s1_t, s2_t)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    return sum(f1_scores)/len(f1_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wiedząc, który model daje najlepsze wyniki dla zbioru walidacyjnego, odpowiedzmy przy jego użyciu na pytania ze zbioru testowego i sprawdźmy rezultaty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 557/557 [14:00<00:00,  1.51s/it]\n"
     ]
    }
   ],
   "source": [
    "lr5e_5_b16_t5_base_answers, expected_answers = exec_passages(lr5e_5_b16_t5_base_model, lr5e_5_b16_t5_base_model_tokenizer, test_passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29442508710801396"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_exact_matches(lr5e_5_b16_t5_base_answers, expected_answers, clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5210907914379148"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_f1(lr5e_5_b16_t5_base_answers, expected_answers, tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otrzymaliśmy następujące wyniki:\n",
    "\n",
    "| lr   | #batchy | eval em | eval f1 |\n",
    "|------|---------|---------|---------|\n",
    "| 5e-5 | 16      | 0.294   | 0.521   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wartości metryk okazały się znacznie słabsze, niż dla zbioru walidacyjnego. Przyjrzyjmy się dziesięcu przykładom bliżej w poszukiwaniu przyczyny gorszych wyników."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Żeby sprawdzić, czy fleksja ma duże znaczenie przy obliczaniu metryki `f1`, stworzę dodatkową funkcję, która poza wyodrębnieniem poszczególnych słów w odpowidzi, dodatkowo zlematyzuje te słowa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pl-core-news-lg==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/pl_core_news_lg-3.8.0/pl_core_news_lg-3.8.0-py3-none-any.whl (573.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m573.7/573.7 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:05\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pl-core-news-lg\n",
      "Successfully installed pl-core-news-lg-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('pl_core_news_lg')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download pl_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"pl_core_news_lg\")\n",
    "\n",
    "def tokenize_and_lemmatize(text):\n",
    "    tokens = tokenize(text)\n",
    "    text = \" \".join(tokens)\n",
    "\n",
    "    doc = nlp(text)\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5395675134388546"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_f1(lr3e_5_b12_t5_base_answers, expected_answers, tokenize_and_lemmatize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okazuje się, że dodatkowa lematyzacja słów dla `f1` tylko nieznacznie poprawia wynik, co jest dla mnie poniekąd zaskoczeniem.\n",
    "\n",
    "Mimo wszystko języki z bogatą fleksją już na poziomie przygotowywania modelu wpływają na jego działanie. Mianowicie model ten musi uwzględnić fleksję zarówno przy tokenizacji, jak i przy budowaniu osadzeń dla wczytanych tokenów. Fleksja prowadzi najpewniej do zwiększenia liczby unikalnych tokenów obsługiwanych przez model, oraz musi zostać uwzględniona w budowie kontekstu dla poszczególnych tokenów, przez co potrzebujemy pojemniejszego modelu i większej liczby zdań w korpusie uczącym, aby dobrze odwzorować zachodzące zależności fleksyjne.\n",
    "\n",
    "Takie dodatkowe wymagania wobec modelu utrudniają jego działanie i przyczyniają się do pogorszenia jakości wyników względem porównywalnego zbioru danych w języku o uboższej fleksji."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| lr   | #batchy | eval em | eval f1 |\n",
    "|------|---------|---------|---------|\n",
    "| 2e-5 | 8       | 0.305   | 0.535   |\n",
    "| 3e-5 | 12      | 0.287   | 0.5257  |\n",
    "| 4e-5 | 16      | 0.293   | 0.5260  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def store_answers_in_json(answers, file_path):\n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(answers, f)\n",
    "\n",
    "def load_answers_from_json(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_answers_in_json(lr5e_5_b16_t5_base_val_answers, \"lr5e_5_b16_t5_base_val_answers.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
