{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marcin Wardyński  \n",
    "czwartek, 8:00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dla przygotowania danych treningowych i walidacyjnych dla modelu trzeba przerobić istniejące pliki json. Format nowych plików powininen zawierać elementy:\n",
    "- id\n",
    "- title\n",
    "- context\n",
    "- question\n",
    "- generative_answer\n",
    "- is_impossible\n",
    "a każdy element powinien zawierać się w pojedyńczym wierszu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def convert_data(data):\n",
    "    results = []\n",
    "    i = 0\n",
    "    for article in data.get(\"data\", []):\n",
    "        for paragraph in article.get(\"paragraphs\", []):\n",
    "            context = paragraph[\"context\"]\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                question = qa[\"question\"]\n",
    "                answers = qa['answers'] if 'answers' in qa.keys() else qa['plausible_answers']\n",
    "                for answer in answers:\n",
    "                    i += 1\n",
    "                    results.append({\n",
    "                        \"id\": i,\n",
    "                        \"context\": context,\n",
    "                        \"question\": question,\n",
    "                        \"answers\": {\n",
    "                            \"text\": [answer[\"generative_answer\"]]\n",
    "                        }\n",
    "                    })\n",
    "    return results\n",
    "\n",
    "\n",
    "def convert_format(input_filepath, output_filepath):\n",
    "    with open(input_filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    output_data = convert_data(data)\n",
    "    output_wrapped_data = {\"version\": \"0.1.0\", \"data\": output_data}\n",
    "\n",
    "    with open(output_filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(output_wrapped_data, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_format(\"poquad-train.json\", \"poquad-conv-train.json\")\n",
    "convert_format(\"poquad-dev.json\", \"poquad-conv-dev.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python run_seq2seq_qa.py \\\n",
    "  --model_name_or_path allegro/plt5-base \\\n",
    "  --dataset_name clarin-pl/poquad \\\n",
    "  --context_column context \\\n",
    "  --question_column question \\\n",
    "  --answer_column answers \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --per_device_train_batch_size 12 \\\n",
    "  --learning_rate 3e-5 \\\n",
    "  --num_train_epochs 3 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --output_dir ../../../../model_poquad_extr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python run_seq2seq_qa.py \\\n",
    "  --model_name_or_path allegro/plt5-base \\\n",
    "  --train_file /Users/mwardynski/Documents/ds/_semestr_9/przetwarzanie_jezyka_naturalnego/labs/lab9/poquad-conv-train.json \\\n",
    "  --validation_file /Users/mwardynski/Documents/ds/_semestr_9/przetwarzanie_jezyka_naturalnego/labs/lab9/poquad-conv-dev.json \\\n",
    "  --context_column context \\\n",
    "  --question_column question \\\n",
    "  --answer_column answers \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --per_device_train_batch_size 12 \\\n",
    "  --learning_rate 3e-5 \\\n",
    "  --num_train_epochs 3 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --output_dir ../../../../model_poquad_abstr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python run_seq2seq_qa.py \\\n",
    "  --model_name_or_path allegro/plt5-base \\\n",
    "  --train_file /Users/mwardynski/Documents/ds/_semestr_9/przetwarzanie_jezyka_naturalnego/labs/lab9/poquad-conv-train.json \\\n",
    "  --validation_file /Users/mwardynski/Documents/ds/_semestr_9/przetwarzanie_jezyka_naturalnego/labs/lab9/poquad-conv-dev.json \\\n",
    "  --context_column context \\\n",
    "  --question_column question \\\n",
    "  --answer_column answers \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --per_device_train_batch_size 16 \\\n",
    "  --learning_rate 2e-5 \\\n",
    "  --num_train_epochs 3 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --output_dir ../../../../model_poquad_t5_base_2e-5_b16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "ready_model_name = \"apohllo/plt5-base-poquad\"\n",
    "ready_model_tokenizer = AutoTokenizer.from_pretrained(ready_model_name)\n",
    "ready_model = AutoModelForSeq2SeqLM.from_pretrained(ready_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Czy żołnierz, który dopuszcza się czynnej napaści na przełożonego podlega karze pozbawienia wolności?\n",
      "Answer: tak\n"
     ]
    }
   ],
   "source": [
    "context = \"Art. 345. § 1. Żołnierz, który dopuszcza się czynnej napaści na przełożonego, podlega karze aresztu wojskowego albo pozbawienia wolności do lat 3. § 2. Jeżeli sprawca dopuszcza się czynnej napaści w związku z pełnieniem przez przełożonego obowiązków służbowych albo wspólnie z innymi żołnierzami lub w obecności zebranych żołnierzy, podlega karze pozbawienia wolności od 6 miesięcy do lat 8. § 3. Jeżeli sprawca czynu określonego w § 1 lub 2 używa broni, noża lub innego podobnie niebezpiecznego przedmiotu, podlega karze pozbawienia wolności od roku do lat 10. § 4. Karze przewidzianej w § 3 podlega sprawca czynu określonego w § 1 lub 2, jeżeli jego następstwem jest skutek określony w art. 156 lub 157 § 1.\"\n",
    "question = \"Czy żołnierz, który dopuszcza się czynnej napaści na przełożonego podlega karze pozbawienia wolności?\"\n",
    "\n",
    "input_text = f\"question: {question} context: {context}\"\n",
    "\n",
    "inputs = ready_model_tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "outputs = ready_model.generate(inputs[\"input_ids\"], max_length=50, num_beams=5, early_stopping=True)\n",
    "\n",
    "answer = ready_model_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "NO_ANS = \"no_ans\"\n",
    "\n",
    "class QA:\n",
    "    def __init__(self, question_id, question, answer):\n",
    "        self.question_id = question_id\n",
    "        self.question = question\n",
    "        self.answer = answer\n",
    "\n",
    "class Entry:\n",
    "    def __init__(self, passage_id, passage_text, qas):\n",
    "        self.passage_id = passage_id\n",
    "        self.passage_text = passage_text\n",
    "        self.qas = qas\n",
    "\n",
    "def init_qas_with_answers(filepath):\n",
    "    qa_dict = {}\n",
    "    with open(filepath, \"r\") as file:\n",
    "        for line in file:\n",
    "            record = json.loads(line.strip())\n",
    "            \n",
    "            if \"score\" in record and record[\"score\"] == \"1\"\\\n",
    "                    and \"question-id\" in record and \"answer\" in record:\n",
    "                qa = QA(record[\"question-id\"], None, record[\"answer\"])\n",
    "                qa_dict[record[\"question-id\"]] = qa\n",
    "    return qa_dict\n",
    "\n",
    "def match_questions_to_answers(filepath, qa_dict):\n",
    "    q_wo_a = []\n",
    "    with open(filepath, \"r\") as file:\n",
    "        for line in file:\n",
    "            record = json.loads(line.strip())\n",
    "            \n",
    "            if \"text\" in record and \"_id\" in record:\n",
    "                if record[\"_id\"] in qa_dict.keys():\n",
    "                    qa = qa_dict[record[\"_id\"]]\n",
    "                    qa.question = record[\"text\"]\n",
    "                else:\n",
    "                    qa_dict[NO_ANS].append(record[\"text\"])\n",
    "\n",
    "\n",
    "def organize_question_to_context_relations(filepath, qa_dict):\n",
    "    qc_dict = {}\n",
    "    with open(filepath, \"r\") as file:\n",
    "        for line in file:\n",
    "            record = json.loads(line.strip())\n",
    "            \n",
    "            if \"score\" in record and record[\"score\"] == \"1\"\\\n",
    "                    and \"passage-id\" in record\\\n",
    "                    and \"question-id\" in record and record[\"question-id\"] in qa_dict.keys():\n",
    "                if record[\"passage-id\"] not in qc_dict.keys():\n",
    "                    qc_dict[record[\"passage-id\"]] = []\n",
    "                qc_dict[record[\"passage-id\"]].append(record[\"question-id\"])\n",
    "    return qc_dict\n",
    "\n",
    "def load_passages(filepath, qc_dict, qa_dict):\n",
    "    entries = []\n",
    "    with open(filepath, \"r\") as file:\n",
    "        for line in file:\n",
    "            record = json.loads(line.strip())\n",
    "            \n",
    "            if \"text\" in record and \"_id\" in record and record[\"_id\"] in qc_dict.keys():\n",
    "                qa_ids = qc_dict[record[\"_id\"]]\n",
    "                qas = []\n",
    "                for qa_id in qa_ids:\n",
    "                    qas.append(qa_dict[qa_id])\n",
    "                entries.append(Entry(record[\"_id\"], record[\"text\"], qas))\n",
    "    return entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_dict = init_qas_with_answers(\"simple-legal-questions-pl-main/answers.jl\")\n",
    "\n",
    "qa_dict[NO_ANS] = []\n",
    "match_questions_to_answers(\"simple-legal-questions-pl-main/questions.jl\", qa_dict)\n",
    "\n",
    "qc_dict = organize_question_to_context_relations(\"simple-legal-questions-pl-main/relevant.jl\", qa_dict)\n",
    "test_passages = load_passages(\"simple-legal-questions-pl-main/passages.jl\", qc_dict, qa_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_poquad_data(filepath):\n",
    "    val_entries = []\n",
    "    with open(filepath, \"r\") as file:\n",
    "        json_content = json.load(file)\n",
    "        for data in json_content['data']:\n",
    "            for paragraph in data['paragraphs']:\n",
    "                qa_list = []\n",
    "                for qa in paragraph['qas']:\n",
    "                    answers = qa['answers'] if 'answers' in qa.keys() else qa['plausible_answers']\n",
    "                    for answer in answers:\n",
    "                        qa_obj = QA(None, qa['question'], answer['generative_answer'])\n",
    "                        qa_list.append(qa_obj)\n",
    "                    \n",
    "                entry = Entry(None, paragraph['context'], qa_list)\n",
    "                val_entries.append(entry)\n",
    "    return val_entries\n",
    "\n",
    "val_passages = convert_poquad_data(\"poquad-dev.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def exec_passages(model, tokenizer, passages):\n",
    "    answers = []\n",
    "    expected_answers = []\n",
    "    for passage in tqdm(passages):\n",
    "        for qa in passage.qas:\n",
    "            input_text = f\"question: {qa.question} context: {passage.passage_text}\"\n",
    "            inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "            outputs = model.generate(inputs[\"input_ids\"], max_length=100, num_beams=5, early_stopping=True)\n",
    "\n",
    "            answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            answers.append(answer)\n",
    "            expected_answers.append(qa.answer)\n",
    "    return answers, expected_answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 557/557 [16:48<00:00,  1.81s/it]\n"
     ]
    }
   ],
   "source": [
    "answers, expected_answers = exec_passages(ready_model, ready_model_tokenizer, test_passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 55/1453 [04:46<2:01:27,  5.21s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m val_answers, expected_answers \u001b[38;5;241m=\u001b[39m \u001b[43mexec_passages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mready_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mready_model_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_passages\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 11\u001b[0m, in \u001b[0;36mexec_passages\u001b[0;34m(model, tokenizer, passages)\u001b[0m\n\u001b[1;32m      8\u001b[0m input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mqa\u001b[38;5;241m.\u001b[39mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m context: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassage\u001b[38;5;241m.\u001b[39mpassage_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(input_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m answer \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m answers\u001b[38;5;241m.\u001b[39mappend(answer)\n",
      "File \u001b[0;32m~/Documents/ds/_semestr_9/przetwarzanie_jezyka_naturalnego/labs/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/ds/_semestr_9/przetwarzanie_jezyka_naturalnego/labs/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:2285\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2277\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2278\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2279\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2280\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2281\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2282\u001b[0m     )\n\u001b[1;32m   2284\u001b[0m     \u001b[38;5;66;03m# 13. run beam sample\u001b[39;00m\n\u001b[0;32m-> 2285\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2286\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2291\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2292\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2293\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2295\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[1;32m   2296\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2297\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2298\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2299\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2305\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2306\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/ds/_semestr_9/przetwarzanie_jezyka_naturalnego/labs/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:3597\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3594\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m outputs\n\u001b[1;32m   3596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3597\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_temporary_reorder_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpast_key_values\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_idx\u001b[49m\n\u001b[1;32m   3599\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3601\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_in_generate \u001b[38;5;129;01mand\u001b[39;00m output_scores:\n\u001b[1;32m   3602\u001b[0m     beam_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m((beam_indices[beam_idx[i]] \u001b[38;5;241m+\u001b[39m (beam_idx[i],) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(beam_indices))))\n",
      "File \u001b[0;32m~/Documents/ds/_semestr_9/przetwarzanie_jezyka_naturalnego/labs/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:3371\u001b[0m, in \u001b[0;36mGenerationMixin._temporary_reorder_cache\u001b[0;34m(self, past_key_values, beam_idx)\u001b[0m\n\u001b[1;32m   3368\u001b[0m     past_key_values \u001b[38;5;241m=\u001b[39m DynamicCache\u001b[38;5;241m.\u001b[39mfrom_legacy_cache(past_key_values)\n\u001b[1;32m   3369\u001b[0m \u001b[38;5;66;03m# Standard code path: use the `Cache.reorder_cache`\u001b[39;00m\n\u001b[1;32m   3370\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3371\u001b[0m     \u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreorder_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeam_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m past_key_values\n",
      "File \u001b[0;32m~/Documents/ds/_semestr_9/przetwarzanie_jezyka_naturalnego/labs/.venv/lib/python3.12/site-packages/transformers/cache_utils.py:1494\u001b[0m, in \u001b[0;36mEncoderDecoderCache.reorder_cache\u001b[0;34m(self, beam_idx)\u001b[0m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreorder_cache\u001b[39m(\u001b[38;5;28mself\u001b[39m, beam_idx: torch\u001b[38;5;241m.\u001b[39mLongTensor):\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Reorders the cache for beam search, given the selected beam indices.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1494\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attention_cache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreorder_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeam_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1495\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attention_cache\u001b[38;5;241m.\u001b[39mreorder_cache(beam_idx)\n",
      "File \u001b[0;32m~/Documents/ds/_semestr_9/przetwarzanie_jezyka_naturalnego/labs/.venv/lib/python3.12/site-packages/transformers/cache_utils.py:100\u001b[0m, in \u001b[0;36mCache.reorder_cache\u001b[0;34m(self, beam_idx)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_cache[layer_idx] \u001b[38;5;241m!=\u001b[39m []:\n\u001b[1;32m     99\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_cache[layer_idx]\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_cache[layer_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_cache[layer_idx]\u001b[38;5;241m.\u001b[39mindex_select(\u001b[38;5;241m0\u001b[39m, \u001b[43mbeam_idx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "val_answers, val_expected_answers = exec_passages(ready_model, ready_model_tokenizer, val_passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "my_t5_base_model_name = \"./model_poquad_abstr\"\n",
    "my_t5_base_model_tokenizer = AutoTokenizer.from_pretrained(my_t5_base_model_name)\n",
    "my_t5_base_model = AutoModelForSeq2SeqLM.from_pretrained(ready_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 557/557 [12:21<00:00,  1.33s/it]\n"
     ]
    }
   ],
   "source": [
    "my_answers, expected_answers = exec_passages(my_t5_base_model, my_t5_base_model_tokenizer, test_passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1453/1453 [2:33:27<00:00,  6.34s/it]  \n"
     ]
    }
   ],
   "source": [
    "my_val_answers, val_expected_answers = exec_passages(my_t5_base_model, my_t5_base_model_tokenizer, val_passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_exact_matches(answers, expected_answers):\n",
    "    matches = 0\n",
    "    for s1, s2 in zip(answers, expected_answers):\n",
    "        if s1.lower() == s2.lower():\n",
    "            matches += 1\n",
    "    return matches/len(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010452961672473868"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_exact_matches(my_answers, expected_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2456445993031359"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_exact_matches(answers, expected_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Sample data\n",
    "list1 = [\"the cat sat on the mat\", \"a dog barked loudly\", \"birds are singing\"]\n",
    "list2 = [\"cat is on the mat\", \"a dog howled loudly\", \"birds are chirping\"]\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    tokens = re.split(r\"[^\\w]+\", text)\n",
    "    return tokens\n",
    "\n",
    "# Compute confusion matrix and F1 score based on token counts\n",
    "def compute_single_f1(tokens1, tokens2):\n",
    "    TP = sum((tokens1 & tokens2).values())\n",
    "    FP = sum((tokens1 - tokens2).values())\n",
    "    FN = sum((tokens2 - tokens1).values())\n",
    "\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return f1\n",
    "\n",
    "def compute_f1(answers, expected_answers, tokenize_fun):\n",
    "\n",
    "    f1_scores = []\n",
    "    for s1, s2 in zip(answers, expected_answers):\n",
    "        s1_t = Counter(tokenize_fun(s1))\n",
    "        s2_t = Counter(tokenize_fun(s2))\n",
    "        f1 = compute_single_f1(s1_t, s2_t)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    return sum(f1_scores)/len(f1_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5184004475490456"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_f1(answers, expected_answers, tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5184004475490456"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_f1(my_answers, expected_answers, tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7216107340920863"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_f1(my_val_answers, val_expected_answers, tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def store_answers_in_json(answers, file_path):\n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(answers, f)\n",
    "\n",
    "def load_answers_from_json(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_answers_in_json(my_answers, \"answers_test_model_poquad_abstr.json\")\n",
    "store_answers_in_json(answer, \"answers_test_plt5-base-poquad.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
