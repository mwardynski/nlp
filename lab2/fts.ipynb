{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marcin Wardyński  \n",
    "czwartek, 8:00\n",
    "\n",
    "## Lab 2\n",
    "\n",
    "Używam zdockeryzowanego Elasticsearch z repozytorium i zakładam, że przed wykonaniem jakichkolwiek zapytań kontener ten zostanie uruchomiony."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Utwórz analizator polskich tekstów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_url = \"http://localhost:9200\"\n",
    "index_name = \"mw_nlp_lab2\"\n",
    "\n",
    "index_url = F\"{es_url}/{index_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'mw_nlp_lab2' deleted successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Delete the index\n",
    "delete_response = requests.delete(f\"{index_url}\")\n",
    "\n",
    "# Check if the deletion was successful\n",
    "if delete_response.status_code == 200:\n",
    "    print(f\"Index '{index_name}' deleted successfully.\")\n",
    "else:\n",
    "    print(f\"Failed to delete index '{index_name}': {delete_response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'[{\"health\":\"yellow\",\"status\":\"open\",\"index\":\"mw_nlp_lab2\",\"uuid\":\"yqa92N2ZSneOi0LMvvYkGQ\",\"pri\":\"1\",\"rep\":\"1\",\"docs.count\":\"57638\",\"docs.deleted\":\"0\",\"store.size\":\"86.7mb\",\"pri.store.size\":\"86.7mb\",\"dataset.size\":\"86.7mb\"}]'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "index_list_response = requests.get(f\"{es_url}/_cat/indices?format=json\")\n",
    "index_list_response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index created.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "fiqa_index_settings = {\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"filter\": {\n",
    "                \"polish_months_synonym\": {\n",
    "                    \"type\": \"synonym\",\n",
    "                    \"synonyms\": [\n",
    "                        \"kwiecień, kwi, IV\",\n",
    "                    ]\n",
    "                },\n",
    "                \"polish_morfologik\": {\n",
    "                    \"type\": \"morfologik_stem\"\n",
    "                }\n",
    "            },\n",
    "            \"analyzer\": {\n",
    "                \"polish_analyzer_1\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\",\n",
    "                        \"polish_months_synonym\",\n",
    "                        \"polish_morfologik\",\n",
    "                        \"lowercase\"\n",
    "                    ]\n",
    "                },\n",
    "                \"polish_analyzer_2\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\",\n",
    "                        \"polish_morfologik\",\n",
    "                        \"lowercase\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fields\": {\n",
    "                    \"analyzed_1\": {\n",
    "                        \"type\": \"text\",\n",
    "                        \"analyzer\": \"polish_analyzer_1\"\n",
    "                    },\n",
    "                    \"analyzed_2\": {\n",
    "                        \"type\": \"text\",\n",
    "                        \"analyzer\": \"polish_analyzer_2\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "  }\n",
    "}\n",
    "\n",
    "response = requests.put(index_url, headers={\"Content-Type\": \"application/json\"}, data=json.dumps(fiqa_index_settings))\n",
    "\n",
    "# Check if the index was created successfully\n",
    "if response.status_code == 200:\n",
    "    print(\"Index created.\")\n",
    "else:\n",
    "    print(f\"Index creation failed: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All documents indexed successfully.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "fiqa_dataset = load_dataset(\"clarin-knext/fiqa-pl\", name=\"corpus\")\n",
    "\n",
    "bulk_data = \"\"\n",
    "for entry in fiqa_dataset[\"corpus\"]:\n",
    "    doc_id = entry[\"_id\"]\n",
    "    bulk_data += json.dumps({\"index\": {\"_index\": index_name, \"_id\": doc_id}}) + \"\\n\"\n",
    "    bulk_data += json.dumps({\"text\": entry['text']}) + \"\\n\"\n",
    "\n",
    "        \n",
    "\n",
    "bulk_response = requests.post(f\"{es_url}/_bulk\", headers={\"Content-Type\": \"application/x-ndjson\"}, data=bulk_data)\n",
    "\n",
    "if bulk_response.status_code == 200:\n",
    "    response_data = bulk_response.json()\n",
    "    if any(item.get(\"index\", {}).get(\"error\") for item in response_data[\"items\"]):\n",
    "        print(\"Some documents failed to index:\")\n",
    "        for item in response_data[\"items\"]:\n",
    "            if \"error\" in item[\"index\"]:\n",
    "                print(item[\"index\"][\"error\"])\n",
    "    else:\n",
    "        print(\"All documents indexed successfully.\")\n",
    "else:\n",
    "    print(f\"Failed to index data: {bulk_response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 257 documents and 353 matches of 'kwiecień' in text.analyzed_2.\n"
     ]
    }
   ],
   "source": [
    "search_word = \"kwiecień\" \n",
    "search_field = \"text.analyzed_2\"\n",
    "\n",
    "max_docs_no = 1000\n",
    "max_highlights_no = 100\n",
    "\n",
    "search_query = {\n",
    "    \"size\": max_docs_no,\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            search_field: search_word\n",
    "        }\n",
    "    },\n",
    "    \"highlight\": {\n",
    "        \"fields\": {\n",
    "            search_field: {\n",
    "                \"type\": \"plain\",\n",
    "                \"fragment_size\": 0,\n",
    "                \"number_of_fragments\": max_highlights_no\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.get(f\"{index_url}/_search\", headers={\"Content-Type\": \"application/json\"}, data=json.dumps(search_query))\n",
    "\n",
    "docs_found = set([])\n",
    "\n",
    "if response.status_code == 200:\n",
    "    search_results = response.json()\n",
    "        \n",
    "    matches_counter = 0\n",
    "    for hit in search_results[\"hits\"][\"hits\"]:\n",
    "        docs_found.add(hit['_id'])\n",
    "        matches_counter += len(hit['highlight'][search_field])\n",
    "\n",
    "    print(f\"Found {search_results['hits']['total']['value']} documents and {matches_counter} matches of '{search_word}' in {search_field}.\")\n",
    "else:\n",
    "    print(f\"Search failed: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Powyższy kod zwraca przy wyszukiwaniu z synoniami 306 dokumentów.\n",
    "Namiast bez synonimów 257 dokumentów.\n",
    "\n",
    "W poprzednim laboratorium mieliśmy za zadanie utworzyć wyrażenie regularne, które znajduje \"kwiecień\" w pełnej odmianie przez przypadki obydwu liczb. Poniżej użyję tego kodu jeszcze raz, żeby sprawdzić, jak się mają jego wyniki z wynikami analizatora bez synonimów. (Porównanie z synonimami nie ma większego sensu, gdyż wyrażenie regularne nie miało ich uwzględniać)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Kwiecien' (directly) found in 265 documents in total 362 times.\n"
     ]
    }
   ],
   "source": [
    "import regex\n",
    "\n",
    "corpus = fiqa_dataset['corpus']\n",
    "\n",
    "april_p = r\"kwie(cień|tni)\"\n",
    "april_pattern = regex.compile(april_p, flags=regex.IGNORECASE | regex.MULTILINE)\n",
    "\n",
    "def count_april_occurrences(what, pattern):\n",
    "    occurrences = {}\n",
    "    counter = 0\n",
    "\n",
    "    for entry in corpus:\n",
    "        found = regex.findall(pattern, entry['text'])\n",
    "        \n",
    "        counter += len(found)\n",
    "        if found:\n",
    "            occurrences[entry[\"_id\"]] = len(found)\n",
    "\n",
    "        \n",
    "        \n",
    "    print(f\"{what} found in {len(occurrences.keys())} documents in total {counter} times.\")\n",
    "    return occurrences\n",
    "\n",
    "\n",
    "occ_april = count_april_occurrences(\"'Kwiecien' (directly)\", april_pattern)\n",
    "regex_docs_found = occ_april.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_found-regex_docs_found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nie ma dokumentów ze słowem bazującym na \"kwiecień\", które by zostało znalezione przez Elasticsearch, ale nie przez wyrażenie regularne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'109292', '159500', '166563', '208216', '265866', '441143', '469888', '82284'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex_docs_found-docs_found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Za to istnieje osiem dokumentów odnalezionych przez wyrażenie regularne, ale nie przez FTS. Bierze się to z faktu, iż wyrażenie regularne zostało sformuowane dość luźno, przez co znajdowało przymiotnik od słowa \"kwiecień\", czyli \"kwietniowy\" i jego pełną fleksję."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def prepare_fiqa_qrels():\n",
    "    subset = 'test'\n",
    "    query_to_corpus_dict = {}\n",
    "\n",
    "    qrels_dataset = load_dataset(\"clarin-knext/fiqa-pl-qrels\")\n",
    "\n",
    "    for item in qrels_dataset[subset]:\n",
    "        if item['query-id'] not in query_to_corpus_dict:\n",
    "            query_to_corpus_dict[item['query-id']] = {}\n",
    "\n",
    "        query_to_corpus_dict[item['query-id']][item['corpus-id']] = item['score']\n",
    "\n",
    "    for query_id in query_to_corpus_dict:\n",
    "        sorted_corpuses_by_score = dict(sorted(query_to_corpus_dict[query_id].items(), key=lambda item: item[1]))\n",
    "        query_to_corpus_dict[query_id] = sorted_corpuses_by_score\n",
    "\n",
    "    return query_to_corpus_dict\n",
    "\n",
    "def prepare_fiqa_queries(query_to_corpus_dict):\n",
    "    queries_dict = {}\n",
    "\n",
    "    queries_dataset = fiqa_dataset['query']\n",
    "    for entry in queries_dataset:\n",
    "        if entry['_id'] in query_to_corpus_dict.keys():\n",
    "            queries_dict[entry['_id']] = entry['text']\n",
    "\n",
    "    return queries_dict\n",
    "\n",
    "\n",
    "query_to_corpus_dict = prepare_fiqa_qrels()\n",
    "queries_dict = prepare_fiqa_queries(query_to_corpus_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_analyze(analyzer, query):\n",
    "    payload = {\n",
    "        \"analyzer\": analyzer,\n",
    "        \"text\": query\n",
    "    }\n",
    "\n",
    "    response = requests.post(f\"{index_url}/_analyze\", headers={\"Content-Type\": \"application/json\"}, data=json.dumps(payload))\n",
    "\n",
    "    # Check if the request was successful and print the response\n",
    "    if response.status_code == 200:\n",
    "        print(\"Analysis result:\", response.json())\n",
    "    else:\n",
    "        print(f\"Failed to analyze text: {response.status_code}\")\n",
    "        print(response.text)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def calculate_ndcg_5(analyzer, query):\n",
    "    elastic_analyze(analyzer, query)\n",
    "    return None\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
