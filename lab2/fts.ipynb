{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marcin Wardyński  \n",
    "czwartek, 8:00\n",
    "\n",
    "## Lab 2\n",
    "\n",
    "Używam zdockeryzowanego Elasticsearch z repozytorium i zakładam, że przed wykonaniem jakichkolwiek zapytań kontener ten zostanie uruchomiony."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Utwórz analizator polskich tekstów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_url = \"http://localhost:9200\"\n",
    "index_name = \"mw_nlp_lab2\"\n",
    "\n",
    "index_url = F\"{es_url}/{index_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'mw_nlp_lab2' deleted successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Delete the index\n",
    "delete_response = requests.delete(f\"{index_url}\")\n",
    "\n",
    "# Check if the deletion was successful\n",
    "if delete_response.status_code == 200:\n",
    "    print(f\"Index '{index_name}' deleted successfully.\")\n",
    "else:\n",
    "    print(f\"Failed to delete index '{index_name}': {delete_response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'[]'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "index_list_response = requests.get(f\"{es_url}/_cat/indices?format=json\")\n",
    "index_list_response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index created.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "fiqa_index_settings = {\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"filter\": {\n",
    "                \"polish_months_synonym\": {\n",
    "                    \"type\": \"synonym\",\n",
    "                    \"synonyms\": [\n",
    "                        \"kwiecień, kwi, IV\",\n",
    "                    ]\n",
    "                },\n",
    "                \"polish_morfologik\": {\n",
    "                    \"type\": \"morfologik_stem\"\n",
    "                }\n",
    "            },\n",
    "            \"analyzer\": {\n",
    "                \"polish_analyzer_1\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\",\n",
    "                        \"polish_months_synonym\",\n",
    "                        \"polish_morfologik\",\n",
    "                        \"lowercase\"\n",
    "                    ]\n",
    "                },\n",
    "                \"polish_analyzer_2\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\",\n",
    "                        \"polish_morfologik\",\n",
    "                        \"lowercase\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fields\": {\n",
    "                    \"analyzed_1\": {\n",
    "                        \"type\": \"text\",\n",
    "                        \"analyzer\": \"polish_analyzer_1\"\n",
    "                    },\n",
    "                    \"analyzed_2\": {\n",
    "                        \"type\": \"text\",\n",
    "                        \"analyzer\": \"polish_analyzer_2\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "  }\n",
    "}\n",
    "\n",
    "response = requests.put(index_url, headers={\"Content-Type\": \"application/json\"}, data=json.dumps(fiqa_index_settings))\n",
    "\n",
    "# Check if the index was created successfully\n",
    "if response.status_code == 200:\n",
    "    print(\"Index created.\")\n",
    "else:\n",
    "    print(f\"Index creation failed: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All documents indexed successfully.\n"
     ]
    }
   ],
   "source": [
    "with open(\"corpus.jsonl\", \"r\") as file:\n",
    "    bulk_data = \"\"\n",
    "    for line in file:\n",
    "        doc = json.loads(line.strip())\n",
    "        doc_id = doc.pop(\"_id\")\n",
    "        bulk_data += json.dumps({\"index\": {\"_index\": index_name, \"_id\": doc_id}}) + \"\\n\"\n",
    "        bulk_data += json.dumps(doc) + \"\\n\"\n",
    "        \n",
    "\n",
    "bulk_response = requests.post(f\"{es_url}/_bulk\", headers={\"Content-Type\": \"application/x-ndjson\"}, data=bulk_data)\n",
    "\n",
    "if bulk_response.status_code == 200:\n",
    "    response_data = bulk_response.json()\n",
    "    if any(item.get(\"index\", {}).get(\"error\") for item in response_data[\"items\"]):\n",
    "        print(\"Some documents failed to index:\")\n",
    "        for item in response_data[\"items\"]:\n",
    "            if \"error\" in item[\"index\"]:\n",
    "                print(item[\"index\"][\"error\"])\n",
    "    else:\n",
    "        print(\"All documents indexed successfully.\")\n",
    "else:\n",
    "    print(f\"Failed to index data: {bulk_response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 257 occurrences of 'kwiecień' in text.analyzed_1.\n"
     ]
    }
   ],
   "source": [
    "search_word = \"kwiecień\"  # Replace with the word you're looking for\n",
    "\n",
    "# Define the search query\n",
    "search_query = {\n",
    "    \"size\": 500,\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"text.analyzed_2\": search_word # text.analyzed_1 for the search with synonims\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Perform the search request\n",
    "response = requests.get(f\"{index_url}/_search\", headers={\"Content-Type\": \"application/json\"}, data=json.dumps(search_query))\n",
    "\n",
    "docs_found = set([])\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    search_results = response.json()\n",
    "    print(f\"Found {search_results['hits']['total']['value']} occurrences of '{search_word}' in text.analyzed_1.\")\n",
    "    \n",
    "    # Print each result\n",
    "    for hit in search_results[\"hits\"][\"hits\"]:\n",
    "        docs_found.add(hit['_id'])\n",
    "else:\n",
    "    print(f\"Search failed: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Powyższy kod zwraca przy wyszukiwaniu z synoniami 306 dokumentów.\n",
    "Namiast bez synonimów 257 dokumentów.\n",
    "\n",
    "W poprzednim laboratorium mieliśmy za zadanie utworzyć wyrażenie regularne, które znajduje \"kwiecień\" w pełnej odmianie przez przypadki obydwu liczb. Poniżej użyję tego kodu jeszcze raz, żeby sprawdzić, jak się mają jego wyniki z wynikami analizatora bez synonimów. (Porównanie z synonimami nie ma większego sensu, gdyż wyrażenie regularne nie miało ich uwzględniać)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "April (directly) found in 265 documents in total 362 times.\n"
     ]
    }
   ],
   "source": [
    "import regex\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"clarin-knext/fiqa-pl\", name=\"corpus\")\n",
    "corpus = dataset['corpus']\n",
    "\n",
    "april_p = r\"kwie(cień|tni)\"\n",
    "april_pattern = regex.compile(april_p, flags=regex.IGNORECASE | regex.MULTILINE)\n",
    "\n",
    "def count_april_occurrences(what, pattern):\n",
    "    occurrences = {}\n",
    "    counter = 0\n",
    "\n",
    "    for entry in corpus:\n",
    "        found = regex.findall(pattern, entry['text'])\n",
    "        \n",
    "        counter += len(found)\n",
    "        if found:\n",
    "            occurrences[entry[\"_id\"]] = len(found)\n",
    "\n",
    "        \n",
    "        \n",
    "    print(f\"{what} found in {len(occurrences.keys())} documents in total {counter} times.\")\n",
    "    return occurrences\n",
    "\n",
    "\n",
    "occ_april = count_april_occurrences(\"April (directly)\", april_pattern)\n",
    "regex_docs_found = occ_april.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_found-regex_docs_found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nie ma dokumentów ze słowem bazującym na \"kwiecień\", które by zostało znalezione przez Elasticsearch, ale nie przez wyrażenie regularne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'109292', '159500', '166563', '208216', '265866', '441143', '469888', '82284'}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex_docs_found-docs_found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Za to istnieje osiem dokumentów odnalezionych przez wyrażenie regularne, ale nie przez FTS. Bierze się to z faktu, iż wyrażenie regularne zostało sformuowane dość luźno, przez co znajdowało przymiotnik od słowa \"kwiecień\", czyli \"kwietniowy\" i jego pełną fleksję."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiqa_pl_qrels.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m---> 10\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mjson\u001b[49m\u001b[38;5;241m.\u001b[39mdumps(item)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "subset = 'train'\n",
    "\n",
    "# Load the FiQA-PL qrels dataset\n",
    "dataset = load_dataset(\"clarin-knext/fiqa-pl-qrels\")\n",
    "\n",
    "with open(f\"fiqa_pl_qrels.jsonl\", \"w\") as f:\n",
    "    for item in dataset['test']:\n",
    "        f.write(f\"{json.dumps(item)}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
